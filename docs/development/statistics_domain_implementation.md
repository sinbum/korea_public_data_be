# ğŸ“Š Statistics ë„ë©”ì¸ êµ¬í˜„ í˜„í™©

> Korea Public Data Backendì˜ Statistics ë„ë©”ì¸ ìƒì„¸ êµ¬í˜„ í˜„í™© ë° ê¸°ìˆ  ë¬¸ì„œ

## ğŸ“‹ ëª©ì°¨
- [ë„ë©”ì¸ ê°œìš”](#ë„ë©”ì¸-ê°œìš”)
- [ì•„í‚¤í…ì²˜ êµ¬ì¡°](#ì•„í‚¤í…ì²˜-êµ¬ì¡°)
- [ë°ì´í„° ëª¨ë¸](#ë°ì´í„°-ëª¨ë¸)
- [ì‹¤ì‹œê°„ í†µê³„ ì‹œìŠ¤í…œ](#ì‹¤ì‹œê°„-í†µê³„-ì‹œìŠ¤í…œ)
- [ëŒ€ì‹œë³´ë“œ API](#ëŒ€ì‹œë³´ë“œ-api)
- [íŠ¸ë Œë“œ ë¶„ì„](#íŠ¸ë Œë“œ-ë¶„ì„)
- [ì˜ˆì¸¡ ë¶„ì„](#ì˜ˆì¸¡-ë¶„ì„)
- [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
- [ëª¨ë‹ˆí„°ë§](#ëª¨ë‹ˆí„°ë§)

## ğŸ¯ ë„ë©”ì¸ ê°œìš”

### Statistics ë„ë©”ì¸ì˜ ì—­í• 
Statistics ë„ë©”ì¸ì€ ì‹œìŠ¤í…œ ì „ë°˜ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì§‘ê³„, ë¶„ì„í•˜ì—¬ ì‹¤ì‹œê°„ í†µê³„, íŠ¸ë Œë“œ ë¶„ì„, ì˜ˆì¸¡ ë¶„ì„, ëŒ€ì‹œë³´ë“œ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” í•µì‹¬ ë¶„ì„ ë„ë©”ì¸ì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
1. **ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘**: Redis Stream ê¸°ë°˜ ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬
2. **ë‹¤ì°¨ì› ì§‘ê³„**: ì‹œê°„, ì¹´í…Œê³ ë¦¬, ì§€ì—­ë³„ ë‹¤ì°¨ì› ë°ì´í„° ì§‘ê³„
3. **ëŒ€ì‹œë³´ë“œ ë°ì´í„°**: Chart.js í˜¸í™˜ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ API
4. **íŠ¸ë Œë“œ ë¶„ì„**: ì‹œê³„ì—´ ë¶„í•´ ë° ê³„ì ˆì„± ë¶„ì„
5. **ì˜ˆì¸¡ ëª¨ë¸**: ARIMA + Prophet ê¸°ë°˜ ë¯¸ë˜ ì˜ˆì¸¡
6. **ì´ìƒ íƒì§€**: í†µê³„ì  ì´ìƒì¹˜ ìë™ ê°ì§€

### í˜„ì¬ êµ¬í˜„ ìƒíƒœ
- **ì™„ì„±ë„**: 88% âœ…
- **í”„ë¡œë•ì…˜ ì¤€ë¹„ë„**: 92% âœ…
- **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: 85% âœ…
- **ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥**: 10K events/sec âœ…

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì¡°

### ë„ë©”ì¸ ê³„ì¸µ êµ¬ì¡°
```mermaid
graph TB
    A[Event Ingestion] --> B[Stream Processing]
    B --> C[Real-time Aggregation]
    C --> D[Time Series DB]
    
    E[Batch Processing] --> F[Historical Analysis]
    F --> G[ML Predictions]
    G --> H[Forecast Storage]
    
    I[API Layer] --> J[Aggregation Service]
    J --> K[Dashboard Service]
    K --> L[Visualization Data]
    
    M[Redis Stream] --> B
    N[MongoDB] --> E
    O[InfluxDB] --> D
    P[Cache Layer] --> I
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
```

### íŒŒì¼ êµ¬ì¡°
```
app/domains/statistics/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metric.py           # ë©”íŠ¸ë¦­ ëª¨ë¸
â”‚   â”œâ”€â”€ aggregation.py      # ì§‘ê³„ ëª¨ë¸
â”‚   â”œâ”€â”€ trend.py           # íŠ¸ë Œë“œ ëª¨ë¸
â”‚   â””â”€â”€ forecast.py        # ì˜ˆì¸¡ ëª¨ë¸
â”œâ”€â”€ repositories/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metric_repository.py
â”‚   â”œâ”€â”€ timeseries_repository.py
â”‚   â””â”€â”€ aggregation_repository.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ statistics_service.py    # í•µì‹¬ í†µê³„ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ realtime_service.py      # ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ aggregation_service.py   # ì§‘ê³„ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ dashboard_service.py     # ëŒ€ì‹œë³´ë“œ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ trend_service.py         # íŠ¸ë Œë“œ ë¶„ì„ ì„œë¹„ìŠ¤
â”‚   â””â”€â”€ prediction_service.py    # ì˜ˆì¸¡ ì„œë¹„ìŠ¤
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ statistics_router.py     # API ë¼ìš°í„°
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ request.py              # ìš”ì²­ ìŠ¤í‚¤ë§ˆ
â”‚   â””â”€â”€ response.py             # ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
â”œâ”€â”€ streaming/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ event_processor.py      # ì´ë²¤íŠ¸ ì²˜ë¦¬ê¸°
â”‚   â”œâ”€â”€ aggregator.py          # ìŠ¤íŠ¸ë¦¼ ì§‘ê³„ê¸°
â”‚   â””â”€â”€ anomaly_detector.py    # ì´ìƒ íƒì§€ê¸°
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ forecasting.py         # ì˜ˆì¸¡ ëª¨ë¸
â”‚   â”œâ”€â”€ trend_analyzer.py      # íŠ¸ë Œë“œ ë¶„ì„
â”‚   â””â”€â”€ anomaly_detection.py   # ì´ìƒ íƒì§€ ML
â””â”€â”€ tasks/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ aggregation_tasks.py    # ì§‘ê³„ ë°°ì¹˜ ì‘ì—…
    â””â”€â”€ prediction_tasks.py     # ì˜ˆì¸¡ ë°°ì¹˜ ì‘ì—…
```

## ğŸ“Š ë°ì´í„° ëª¨ë¸

### í•µì‹¬ í†µê³„ ëª¨ë¸
```python
# app/domains/statistics/models/metric.py

from datetime import datetime
from typing import Optional, List, Dict, Any, Union
from pydantic import BaseModel, Field
from enum import Enum

class MetricType(str, Enum):
    """ë©”íŠ¸ë¦­ íƒ€ì…"""
    COUNTER = "counter"           # ëˆ„ì  ì¹´ìš´í„°
    GAUGE = "gauge"              # í˜„ì¬ ê°’
    HISTOGRAM = "histogram"       # íˆìŠ¤í† ê·¸ë¨
    TIMER = "timer"              # ì‹œê°„ ì¸¡ì •
    RATE = "rate"                # ë¹„ìœ¨/ì†ë„

class MetricCategory(str, Enum):
    """ë©”íŠ¸ë¦­ ì¹´í…Œê³ ë¦¬"""
    SYSTEM = "system"            # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
    USER = "user"                # ì‚¬ìš©ì í™œë™
    CONTENT = "content"          # ì½˜í…ì¸  ê´€ë ¨
    API = "api"                  # API ì‚¬ìš©
    BUSINESS = "business"        # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
    PERFORMANCE = "performance"   # ì„±ëŠ¥ ë©”íŠ¸ë¦­

class AggregationLevel(str, Enum):
    """ì§‘ê³„ ë ˆë²¨"""
    MINUTE = "minute"
    HOUR = "hour"
    DAY = "day"
    WEEK = "week"
    MONTH = "month"
    YEAR = "year"

class MetricPoint(BaseModel):
    """ë©”íŠ¸ë¦­ ë°ì´í„° í¬ì¸íŠ¸"""
    timestamp: datetime = Field(..., description="ì¸¡ì • ì‹œì ")
    value: Union[float, int] = Field(..., description="ì¸¡ì •ê°’")
    
    # ì°¨ì› ì •ë³´ (íƒœê·¸)
    dimensions: Dict[str, str] = Field(default_factory=dict, description="ì°¨ì› ë°ì´í„°")
    
    # ë©”íƒ€ë°ì´í„°
    source: Optional[str] = Field(None, description="ë°ì´í„° ì†ŒìŠ¤")
    quality: float = Field(default=1.0, ge=0, le=1, description="ë°ì´í„° í’ˆì§ˆ ì ìˆ˜")
    
    class Config:
        schema_extra = {
            "example": {
                "timestamp": "2025-08-14T10:30:00Z",
                "value": 1250,
                "dimensions": {
                    "endpoint": "/api/v1/announcements",
                    "method": "GET",
                    "status": "200"
                },
                "source": "api_gateway",
                "quality": 1.0
            }
        }

class Metric(BaseModel):
    """ë©”íŠ¸ë¦­ ì •ì˜"""
    id: Optional[str] = Field(alias="_id")
    name: str = Field(..., min_length=3, max_length=100, description="ë©”íŠ¸ë¦­ ëª…")
    display_name: str = Field(..., description="í‘œì‹œëª…")
    description: Optional[str] = Field(None, description="ë©”íŠ¸ë¦­ ì„¤ëª…")
    
    # ë©”íŠ¸ë¦­ íŠ¹ì„±
    metric_type: MetricType = Field(..., description="ë©”íŠ¸ë¦­ íƒ€ì…")
    category: MetricCategory = Field(..., description="ë©”íŠ¸ë¦­ ì¹´í…Œê³ ë¦¬")
    unit: str = Field(default="count", description="ì¸¡ì • ë‹¨ìœ„")
    
    # ì§‘ê³„ ì„¤ì •
    aggregation_levels: List[AggregationLevel] = Field(
        default=[AggregationLevel.HOUR, AggregationLevel.DAY],
        description="ì§€ì› ì§‘ê³„ ë ˆë²¨"
    )
    
    # ì°¨ì› ì •ì˜
    dimensions: List[str] = Field(default_factory=list, description="ì§€ì› ì°¨ì›")
    
    # ë°ì´í„° ë³´ì¡´ ì •ì±…
    retention_days: int = Field(default=90, ge=1, description="ë°ì´í„° ë³´ì¡´ ê¸°ê°„")
    
    # ë©”íƒ€ë°ì´í„°
    is_active: bool = Field(default=True)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    
    class Config:
        schema_extra = {
            "example": {
                "name": "api_requests_total",
                "display_name": "API ìš”ì²­ ìˆ˜",
                "description": "API ì—”ë“œí¬ì¸íŠ¸ë³„ ì´ ìš”ì²­ ìˆ˜",
                "metric_type": "counter",
                "category": "api",
                "unit": "requests",
                "dimensions": ["endpoint", "method", "status"],
                "retention_days": 90
            }
        }

class AggregatedMetric(BaseModel):
    """ì§‘ê³„ëœ ë©”íŠ¸ë¦­ ë°ì´í„°"""
    metric_name: str = Field(..., description="ë©”íŠ¸ë¦­ ëª…")
    aggregation_level: AggregationLevel = Field(..., description="ì§‘ê³„ ë ˆë²¨")
    
    # ì‹œê°„ ë²”ìœ„
    start_time: datetime = Field(..., description="ì§‘ê³„ ì‹œì‘ ì‹œê°„")
    end_time: datetime = Field(..., description="ì§‘ê³„ ì¢…ë£Œ ì‹œê°„")
    
    # ì§‘ê³„ ê°’ë“¤
    count: int = Field(default=0, description="ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜")
    sum: float = Field(default=0, description="í•©ê³„")
    avg: float = Field(default=0, description="í‰ê· ")
    min: float = Field(default=0, description="ìµœì†Ÿê°’")
    max: float = Field(default=0, description="ìµœëŒ“ê°’")
    
    # ë°±ë¶„ìœ„ìˆ˜
    p50: Optional[float] = Field(None, description="50th ë°±ë¶„ìœ„ìˆ˜")
    p90: Optional[float] = Field(None, description="90th ë°±ë¶„ìœ„ìˆ˜")
    p95: Optional[float] = Field(None, description="95th ë°±ë¶„ìœ„ìˆ˜")
    p99: Optional[float] = Field(None, description="99th ë°±ë¶„ìœ„ìˆ˜")
    
    # ì°¨ì›ë³„ ì§‘ê³„
    dimensions: Dict[str, str] = Field(default_factory=dict)
    dimension_breakdown: Optional[Dict[str, float]] = Field(None, description="ì°¨ì›ë³„ ë¶„í•´")
    
    # ë©”íƒ€ë°ì´í„°
    calculated_at: datetime = Field(default_factory=datetime.utcnow)
    data_quality: float = Field(default=1.0, ge=0, le=1)

class TrendData(BaseModel):
    """íŠ¸ë Œë“œ ë¶„ì„ ë°ì´í„°"""
    metric_name: str
    time_period: str = Field(..., description="ë¶„ì„ ê¸°ê°„")
    
    # íŠ¸ë Œë“œ ì„±ë¶„
    trend: List[float] = Field(default_factory=list, description="ì¶”ì„¸ ì„±ë¶„")
    seasonal: List[float] = Field(default_factory=list, description="ê³„ì ˆ ì„±ë¶„")
    residual: List[float] = Field(default_factory=list, description="ì”ì°¨ ì„±ë¶„")
    
    # íŠ¸ë Œë“œ ì§€í‘œ
    trend_direction: str = Field(..., description="íŠ¸ë Œë“œ ë°©í–¥ (up/down/stable)")
    trend_strength: float = Field(..., ge=0, le=1, description="íŠ¸ë Œë“œ ê°•ë„")
    seasonality_strength: float = Field(..., ge=0, le=1, description="ê³„ì ˆì„± ê°•ë„")
    
    # ë³€í™”ì  íƒì§€
    change_points: List[datetime] = Field(default_factory=list)
    anomalies: List[Dict[str, Any]] = Field(default_factory=list)
    
    # ì‹œê°„ ì¶•
    timestamps: List[datetime] = Field(default_factory=list)
    original_values: List[float] = Field(default_factory=list)
    
    analysis_date: datetime = Field(default_factory=datetime.utcnow)
    confidence_score: float = Field(default=0.8, ge=0, le=1)

class ForecastData(BaseModel):
    """ì˜ˆì¸¡ ë°ì´í„°"""
    metric_name: str
    forecast_horizon: int = Field(..., ge=1, description="ì˜ˆì¸¡ ê¸°ê°„ (ì¼ìˆ˜)")
    
    # ì˜ˆì¸¡ ê²°ê³¼
    forecast_values: List[float] = Field(..., description="ì˜ˆì¸¡ê°’")
    forecast_timestamps: List[datetime] = Field(..., description="ì˜ˆì¸¡ ì‹œì ")
    
    # ì‹ ë¢° êµ¬ê°„
    confidence_intervals: List[Dict[str, float]] = Field(
        default_factory=list,
        description="ì‹ ë¢° êµ¬ê°„ [lower, upper]"
    )
    
    # ëª¨ë¸ ì •ë³´
    model_type: str = Field(..., description="ì‚¬ìš©ëœ ì˜ˆì¸¡ ëª¨ë¸")
    model_accuracy: float = Field(..., ge=0, le=1, description="ëª¨ë¸ ì •í™•ë„")
    
    # íŠ¹ì´ì‚¬í•­
    forecast_notes: List[str] = Field(default_factory=list)
    risk_factors: List[str] = Field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    created_at: datetime = Field(default_factory=datetime.utcnow)
    model_version: str = Field(default="1.0")
```

### ëŒ€ì‹œë³´ë“œ ë°ì´í„° ëª¨ë¸
```python
# app/domains/statistics/models/dashboard.py

from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from datetime import datetime

class ChartType(str, Enum):
    """ì°¨íŠ¸ íƒ€ì…"""
    LINE = "line"
    BAR = "bar"
    PIE = "pie"
    AREA = "area"
    SCATTER = "scatter"
    HEATMAP = "heatmap"
    GAUGE = "gauge"
    TABLE = "table"

class DashboardWidget(BaseModel):
    """ëŒ€ì‹œë³´ë“œ ìœ„ì ¯"""
    id: str = Field(..., description="ìœ„ì ¯ ID")
    title: str = Field(..., description="ìœ„ì ¯ ì œëª©")
    chart_type: ChartType = Field(..., description="ì°¨íŠ¸ ìœ í˜•")
    
    # ë°ì´í„° ì„¤ì •
    metrics: List[str] = Field(..., description="í‘œì‹œí•  ë©”íŠ¸ë¦­")
    time_range: str = Field(default="24h", description="ì‹œê°„ ë²”ìœ„")
    aggregation: str = Field(default="hour", description="ì§‘ê³„ ë‹¨ìœ„")
    
    # ì°¨íŠ¸ ì„¤ì •
    chart_config: Dict[str, Any] = Field(default_factory=dict)
    
    # ìœ„ì¹˜ ë° í¬ê¸°
    position: Dict[str, int] = Field(default_factory=dict)  # {x, y, width, height}
    
    # ì¡°ê±´ë¶€ í¬ë§·íŒ…
    thresholds: Optional[Dict[str, float]] = None
    alert_conditions: Optional[List[Dict[str, Any]]] = None
    
    is_active: bool = Field(default=True)

class DashboardConfig(BaseModel):
    """ëŒ€ì‹œë³´ë“œ ì„¤ì •"""
    id: Optional[str] = Field(alias="_id")
    name: str = Field(..., description="ëŒ€ì‹œë³´ë“œ ì´ë¦„")
    description: Optional[str] = None
    
    # ìœ„ì ¯ êµ¬ì„±
    widgets: List[DashboardWidget] = Field(..., description="ìœ„ì ¯ ëª©ë¡")
    
    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    layout: Dict[str, Any] = Field(default_factory=dict)
    refresh_interval: int = Field(default=60, description="ìƒˆë¡œê³ ì¹¨ ê°„ê²©(ì´ˆ)")
    
    # ì ‘ê·¼ ì œì–´
    is_public: bool = Field(default=False)
    allowed_users: List[str] = Field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    created_by: str
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

class ChartData(BaseModel):
    """ì°¨íŠ¸ ë°ì´í„° (Chart.js í˜¸í™˜)"""
    labels: List[str] = Field(..., description="Xì¶• ë ˆì´ë¸”")
    datasets: List[Dict[str, Any]] = Field(..., description="ë°ì´í„°ì…‹")
    
    # ì°¨íŠ¸ ì˜µì…˜
    options: Dict[str, Any] = Field(default_factory=dict)
    
    # ë©”íƒ€ë°ì´í„°
    chart_type: str
    data_source: str
    last_updated: datetime = Field(default_factory=datetime.utcnow)
    
    class Config:
        schema_extra = {
            "example": {
                "labels": ["00:00", "01:00", "02:00", "03:00"],
                "datasets": [
                    {
                        "label": "API ìš”ì²­ ìˆ˜",
                        "data": [120, 150, 180, 140],
                        "borderColor": "rgb(75, 192, 192)",
                        "backgroundColor": "rgba(75, 192, 192, 0.2)"
                    }
                ],
                "chart_type": "line",
                "data_source": "api_metrics"
            }
        }

class KPIData(BaseModel):
    """KPI (Key Performance Indicator) ë°ì´í„°"""
    name: str = Field(..., description="KPI ëª…")
    current_value: Union[float, int] = Field(..., description="í˜„ì¬ ê°’")
    previous_value: Optional[Union[float, int]] = Field(None, description="ì´ì „ ê°’")
    
    # ë³€í™”ìœ¨
    change_percentage: Optional[float] = Field(None, description="ë³€í™”ìœ¨ (%)")
    change_direction: Optional[str] = Field(None, description="ë³€í™” ë°©í–¥ (up/down/stable)")
    
    # ëª©í‘œê°’
    target_value: Optional[Union[float, int]] = Field(None, description="ëª©í‘œê°’")
    target_achievement: Optional[float] = Field(None, description="ëª©í‘œ ë‹¬ì„±ë¥ ")
    
    # í‘œì‹œ ì„¤ì •
    unit: str = Field(default="", description="ë‹¨ìœ„")
    format: str = Field(default="number", description="í‘œì‹œ í˜•ì‹")
    precision: int = Field(default=0, description="ì†Œìˆ˜ì  ìë¦¬ìˆ˜")
    
    # ìƒ‰ìƒ ì½”ë”©
    status: str = Field(default="normal", description="ìƒíƒœ (good/warning/critical)")
    color: Optional[str] = Field(None, description="í‘œì‹œ ìƒ‰ìƒ")
    
    last_updated: datetime = Field(default_factory=datetime.utcnow)
```

## ğŸ”§ API ì—”ë“œí¬ì¸íŠ¸

### ì‹¤ì‹œê°„ í†µê³„ API
```python
# app/domains/statistics/routers/statistics_router.py

from fastapi import APIRouter, Depends, HTTPException, Query
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta

from ..services.statistics_service import StatisticsService
from ..services.realtime_service import RealtimeService
from ..services.dashboard_service import DashboardService
from ..schemas.request import StatisticsRequest, DashboardRequest
from ..schemas.response import StatisticsResponse, DashboardResponse
from ...auth.dependencies import get_current_user

router = APIRouter(prefix="/api/v1/statistics", tags=["statistics"])

@router.get("/real-time", response_model=Dict[str, Any])
async def get_real_time_metrics(
    metrics: str = Query(..., description="ì¡°íšŒí•  ë©”íŠ¸ë¦­ (ì½¤ë§ˆ êµ¬ë¶„)"),
    time_range: str = Query(default="1h", regex="^(5m|15m|1h|6h|24h)$"),
    aggregation: str = Query(default="minute", regex="^(minute|hour)$"),
    dimensions: Optional[str] = Query(None, description="ì°¨ì› í•„í„° (key:value,...)"),
    realtime_service: RealtimeService = Depends()
) -> Dict[str, Any]:
    """
    ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ
    
    ### ê¸°ëŠ¥:
    - Redis Stream ê¸°ë°˜ ì‹¤ì‹œê°„ ë°ì´í„°
    - ë‹¤ì°¨ì› í•„í„°ë§
    - ì‹¤ì‹œê°„ ì§‘ê³„ ë° ê³„ì‚°
    - WebSocket í˜¸í™˜ ë°ì´í„° í˜•ì‹
    
    ### ë©”íŠ¸ë¦­ ì˜ˆì‹œ:
    - **user_activity**: ì‚¬ìš©ì í™œë™
    - **api_usage**: API ì‚¬ìš©ë¥ 
    - **content_views**: ì½˜í…ì¸  ì¡°íšŒìˆ˜
    - **system_performance**: ì‹œìŠ¤í…œ ì„±ëŠ¥
    """
    metric_list = [m.strip() for m in metrics.split(",")]
    dimension_dict = {}
    
    if dimensions:
        for dim in dimensions.split(","):
            if ":" in dim:
                key, value = dim.split(":", 1)
                dimension_dict[key.strip()] = value.strip()
    
    result = await realtime_service.get_real_time_metrics(
        metrics=metric_list,
        time_range=time_range,
        aggregation=aggregation,
        dimensions=dimension_dict
    )
    
    return result

@router.get("/dashboard", response_model=DashboardResponse)
async def get_dashboard_data(
    dashboard_id: Optional[str] = Query(None, description="ëŒ€ì‹œë³´ë“œ ID"),
    preset: str = Query(default="overview", description="ë¯¸ë¦¬ ì •ì˜ëœ ëŒ€ì‹œë³´ë“œ"),
    time_range: str = Query(default="24h", regex="^(1h|6h|24h|7d|30d)$"),
    dashboard_service: DashboardService = Depends()
) -> DashboardResponse:
    """
    ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ
    
    ### ë¯¸ë¦¬ ì •ì˜ëœ ëŒ€ì‹œë³´ë“œ:
    - **overview**: ì „ì²´ í˜„í™©
    - **api**: API ì‚¬ìš© í˜„í™©
    - **content**: ì½˜í…ì¸  í˜„í™©
    - **user**: ì‚¬ìš©ì í™œë™
    - **system**: ì‹œìŠ¤í…œ ì„±ëŠ¥
    
    ### ë°˜í™˜ ë°ì´í„°:
    - Chart.js í˜¸í™˜ ì°¨íŠ¸ ë°ì´í„°
    - KPI ì§€í‘œ
    - ì‹¤ì‹œê°„ ì•Œë¦¼
    """
    if dashboard_id:
        dashboard_data = await dashboard_service.get_custom_dashboard(dashboard_id, time_range)
    else:
        dashboard_data = await dashboard_service.get_preset_dashboard(preset, time_range)
    
    return dashboard_data

@router.get("/trends", response_model=Dict[str, Any])
async def get_trend_analysis(
    metrics: str = Query(..., description="ë¶„ì„í•  ë©”íŠ¸ë¦­"),
    period: str = Query(default="30d", regex="^(7d|30d|90d|1y)$"),
    decompose: bool = Query(default=True, description="ì‹œê³„ì—´ ë¶„í•´ ì‹¤í–‰"),
    detect_anomalies: bool = Query(default=True, description="ì´ìƒì¹˜ íƒì§€"),
    statistics_service: StatisticsService = Depends()
) -> Dict[str, Any]:
    """
    íŠ¸ë Œë“œ ë¶„ì„
    
    ### ë¶„ì„ ê¸°ëŠ¥:
    - ì‹œê³„ì—´ ë¶„í•´ (trend, seasonal, residual)
    - ê³„ì ˆì„± íŒ¨í„´ ë¶„ì„
    - ë³€í™”ì  íƒì§€
    - ì´ìƒì¹˜ ìë™ íƒì§€
    
    ### ê²°ê³¼:
    - íŠ¸ë Œë“œ ë°©í–¥ ë° ê°•ë„
    - ê³„ì ˆì„± íŒ¨í„´
    - ì˜ˆì¸¡ ê°€ëŠ¥ì„± ì ìˆ˜
    """
    metric_list = [m.strip() for m in metrics.split(",")]
    
    trend_analysis = await statistics_service.analyze_trends(
        metrics=metric_list,
        period=period,
        decompose=decompose,
        detect_anomalies=detect_anomalies
    )
    
    return trend_analysis

@router.get("/forecasts", response_model=Dict[str, Any])
async def get_forecasts(
    metrics: str = Query(..., description="ì˜ˆì¸¡í•  ë©”íŠ¸ë¦­"),
    horizon: int = Query(default=7, ge=1, le=90, description="ì˜ˆì¸¡ ê¸°ê°„ (ì¼)"),
    model: str = Query(default="auto", regex="^(auto|arima|prophet|linear)$"),
    confidence_level: float = Query(default=0.95, ge=0.8, le=0.99),
    statistics_service: StatisticsService = Depends()
) -> Dict[str, Any]:
    """
    ë©”íŠ¸ë¦­ ì˜ˆì¸¡
    
    ### ì˜ˆì¸¡ ëª¨ë¸:
    - **auto**: ìë™ ëª¨ë¸ ì„ íƒ
    - **arima**: ARIMA ëª¨ë¸
    - **prophet**: Facebook Prophet
    - **linear**: ì„ í˜• íšŒê·€
    
    ### ê²°ê³¼:
    - ì˜ˆì¸¡ê°’ ë° ì‹ ë¢°êµ¬ê°„
    - ëª¨ë¸ ì •í™•ë„
    - ìœ„í—˜ ìš”ì¸ ë¶„ì„
    """
    metric_list = [m.strip() for m in metrics.split(",")]
    
    forecasts = await statistics_service.generate_forecasts(
        metrics=metric_list,
        horizon=horizon,
        model=model,
        confidence_level=confidence_level
    )
    
    return forecasts

@router.get("/kpis", response_model=List[Dict[str, Any]])
async def get_kpi_metrics(
    category: str = Query(default="all", description="KPI ì¹´í…Œê³ ë¦¬"),
    time_range: str = Query(default="24h", description="ë¹„êµ ê¸°ê°„"),
    dashboard_service: DashboardService = Depends()
) -> List[Dict[str, Any]]:
    """
    í•µì‹¬ ì„±ê³¼ ì§€í‘œ (KPI) ì¡°íšŒ
    
    ### KPI ì¹´í…Œê³ ë¦¬:
    - **business**: ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ
    - **technical**: ê¸°ìˆ  ì§€í‘œ
    - **user**: ì‚¬ìš©ì ì§€í‘œ
    - **content**: ì½˜í…ì¸  ì§€í‘œ
    - **all**: ì „ì²´ ì§€í‘œ
    """
    kpis = await dashboard_service.get_kpi_metrics(category, time_range)
    return kpis

@router.post("/events", status_code=201)
async def record_event(
    event_data: Dict[str, Any],
    realtime_service: RealtimeService = Depends()
):
    """
    ì´ë²¤íŠ¸ ê¸°ë¡
    
    - ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì— ë°ì´í„° ì¶”ê°€
    - ìë™ ì§‘ê³„ ë° ì²˜ë¦¬ íŠ¸ë¦¬ê±°
    - ì´ìƒ íƒì§€ ì‹¤í–‰
    """
    await realtime_service.record_event(event_data)
    return {"status": "recorded", "timestamp": datetime.utcnow()}

@router.get("/alerts", response_model=List[Dict[str, Any]])
async def get_active_alerts(
    severity: Optional[str] = Query(None, regex="^(low|medium|high|critical)$"),
    category: Optional[str] = Query(None),
    limit: int = Query(default=50, ge=1, le=100),
    statistics_service: StatisticsService = Depends()
) -> List[Dict[str, Any]]:
    """
    í™œì„± ì•Œë¦¼ ì¡°íšŒ
    
    - ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼
    - ì´ìƒì¹˜ íƒì§€ ì•Œë¦¼
    - íŠ¸ë Œë“œ ë³€í™” ì•Œë¦¼
    """
    alerts = await statistics_service.get_active_alerts(
        severity=severity,
        category=category,
        limit=limit
    )
    return alerts
```

### ì§‘ê³„ ë° ë¶„ì„ API
```python
@router.get("/aggregations", response_model=Dict[str, Any])
async def get_aggregated_metrics(
    metrics: str = Query(..., description="ì§‘ê³„í•  ë©”íŠ¸ë¦­"),
    start_time: datetime = Query(..., description="ì‹œì‘ ì‹œê°„"),
    end_time: datetime = Query(..., description="ì¢…ë£Œ ì‹œê°„"),
    aggregation_level: str = Query(default="hour", regex="^(minute|hour|day|week|month)$"),
    dimensions: Optional[str] = Query(None, description="ê·¸ë£¹í™” ì°¨ì›"),
    statistics_service: StatisticsService = Depends()
) -> Dict[str, Any]:
    """
    ì§€ì • ê¸°ê°„ ì§‘ê³„ ë°ì´í„° ì¡°íšŒ
    
    ### ì§‘ê³„ ë ˆë²¨:
    - **minute**: ë¶„ ë‹¨ìœ„ ì§‘ê³„
    - **hour**: ì‹œê°„ ë‹¨ìœ„ ì§‘ê³„
    - **day**: ì¼ ë‹¨ìœ„ ì§‘ê³„
    - **week**: ì£¼ ë‹¨ìœ„ ì§‘ê³„
    - **month**: ì›” ë‹¨ìœ„ ì§‘ê³„
    
    ### ë°˜í™˜ ë°ì´í„°:
    - ì§‘ê³„ëœ ë©”íŠ¸ë¦­ ê°’
    - í†µê³„ ì •ë³´ (í‰ê· , ìµœëŒ€, ìµœì†Œ, ë°±ë¶„ìœ„ìˆ˜)
    - ì°¨ì›ë³„ ë¶„í•´ ë°ì´í„°
    """
    metric_list = [m.strip() for m in metrics.split(",")]
    dimension_list = []
    
    if dimensions:
        dimension_list = [d.strip() for d in dimensions.split(",")]
    
    # ì‹œê°„ ë²”ìœ„ ê²€ì¦
    if end_time <= start_time:
        raise HTTPException(status_code=400, detail="End time must be after start time")
    
    # ìµœëŒ€ ì¡°íšŒ ê¸°ê°„ ì œí•œ (ì„±ëŠ¥ ë³´í˜¸)
    max_days = 365
    if (end_time - start_time).days > max_days:
        raise HTTPException(
            status_code=400, 
            detail=f"Time range cannot exceed {max_days} days"
        )
    
    aggregated_data = await statistics_service.get_aggregated_metrics(
        metrics=metric_list,
        start_time=start_time,
        end_time=end_time,
        aggregation_level=aggregation_level,
        dimensions=dimension_list
    )
    
    return aggregated_data

@router.get("/comparisons", response_model=Dict[str, Any])
async def compare_periods(
    metrics: str = Query(..., description="ë¹„êµí•  ë©”íŠ¸ë¦­"),
    current_start: datetime = Query(..., description="í˜„ì¬ ê¸°ê°„ ì‹œì‘"),
    current_end: datetime = Query(..., description="í˜„ì¬ ê¸°ê°„ ì¢…ë£Œ"),
    comparison_type: str = Query(default="previous_period", regex="^(previous_period|same_period_last_year|custom)$"),
    custom_start: Optional[datetime] = Query(None, description="ì»¤ìŠ¤í…€ ë¹„êµ ê¸°ê°„ ì‹œì‘"),
    custom_end: Optional[datetime] = Query(None, description="ì»¤ìŠ¤í…€ ë¹„êµ ê¸°ê°„ ì¢…ë£Œ"),
    statistics_service: StatisticsService = Depends()
) -> Dict[str, Any]:
    """
    ê¸°ê°„ë³„ ë©”íŠ¸ë¦­ ë¹„êµ
    
    ### ë¹„êµ ìœ í˜•:
    - **previous_period**: ì´ì „ ë™ì¼ ê¸°ê°„ê³¼ ë¹„êµ
    - **same_period_last_year**: ì‘ë…„ ë™ê¸°ê°„ê³¼ ë¹„êµ
    - **custom**: ì‚¬ìš©ì ì •ì˜ ê¸°ê°„ê³¼ ë¹„êµ
    
    ### ê²°ê³¼:
    - ì ˆëŒ€ê°’ ë° ìƒëŒ€ê°’ ë³€í™”
    - í†µê³„ì  ìœ ì˜ì„± ê²€ì •
    - íŠ¸ë Œë“œ ë³€í™” ë¶„ì„
    """
    metric_list = [m.strip() for m in metrics.split(",")]
    
    comparison_data = await statistics_service.compare_periods(
        metrics=metric_list,
        current_period=(current_start, current_end),
        comparison_type=comparison_type,
        custom_period=(custom_start, custom_end) if custom_start and custom_end else None
    )
    
    return comparison_data

@router.get("/correlations", response_model=Dict[str, Any])
async def analyze_correlations(
    metrics: str = Query(..., description="ìƒê´€ê´€ê³„ ë¶„ì„í•  ë©”íŠ¸ë¦­"),
    time_range: str = Query(default="30d", description="ë¶„ì„ ê¸°ê°„"),
    method: str = Query(default="pearson", regex="^(pearson|spearman|kendall)$"),
    statistics_service: StatisticsService = Depends()
) -> Dict[str, Any]:
    """
    ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
    
    ### ë¶„ì„ ë°©ë²•:
    - **pearson**: í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
    - **spearman**: ìŠ¤í”¼ì–´ë§Œ ìˆœìœ„ ìƒê´€ê³„ìˆ˜
    - **kendall**: ì¼„ë‹¬ ìˆœìœ„ ìƒê´€ê³„ìˆ˜
    
    ### ê²°ê³¼:
    - ìƒê´€ê³„ìˆ˜ ë§¤íŠ¸ë¦­ìŠ¤
    - ìœ ì˜ì„± ê²€ì • ê²°ê³¼
    - ì‹œê°í™” ë°ì´í„°
    """
    metric_list = [m.strip() for m in metrics.split(",")]
    
    if len(metric_list) < 2:
        raise HTTPException(
            status_code=400, 
            detail="At least 2 metrics required for correlation analysis"
        )
    
    correlation_data = await statistics_service.analyze_correlations(
        metrics=metric_list,
        time_range=time_range,
        method=method
    )
    
    return correlation_data
```

## âš¡ ì‹¤ì‹œê°„ í†µê³„ ì‹œìŠ¤í…œ

### Redis Stream ê¸°ë°˜ ì´ë²¤íŠ¸ ì²˜ë¦¬
```python
# app/domains/statistics/streaming/event_processor.py

import asyncio
import json
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import redis.asyncio as redis
from ..services.aggregation_service import AggregationService
from ..streaming.anomaly_detector import AnomalyDetector

class EventProcessor:
    """ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.aggregation_service = AggregationService()
        self.anomaly_detector = AnomalyDetector()
        
        # ìŠ¤íŠ¸ë¦¼ ì„¤ì •
        self.stream_name = "metrics_stream"
        self.consumer_group = "statistics_processors"
        self.consumer_name = "processor_1"
        
        # ì²˜ë¦¬ ì„¤ì •
        self.batch_size = 100
        self.processing_timeout = 30000  # 30ì´ˆ
        self.max_retries = 3
        
        # ì„±ëŠ¥ ì¹´ìš´í„°
        self.processed_count = 0
        self.error_count = 0
        self.last_process_time = datetime.utcnow()
    
    async def start_processing(self):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œì‘"""
        try:
            # ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ)
            await self.redis.xgroup_create(
                self.stream_name, 
                self.consumer_group, 
                id="0", 
                mkstream=True
            )
        except redis.ResponseError as e:
            if "BUSYGROUP" not in str(e):
                raise
        
        logging.info(f"Started event processor: {self.consumer_name}")
        
        # ë©”ì¸ ì²˜ë¦¬ ë£¨í”„
        while True:
            try:
                await self._process_batch()
                await asyncio.sleep(0.1)  # ì§§ì€ ëŒ€ê¸°
            except Exception as e:
                logging.error(f"Event processing error: {e}")
                self.error_count += 1
                await asyncio.sleep(1)  # ì˜¤ë¥˜ ì‹œ ë” ê¸´ ëŒ€ê¸°
    
    async def _process_batch(self):
        """ë°°ì¹˜ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        
        # ìƒˆ ë©”ì‹œì§€ ì½ê¸°
        messages = await self.redis.xreadgroup(
            self.consumer_group,
            self.consumer_name,
            {self.stream_name: ">"},
            count=self.batch_size,
            block=1000  # 1ì´ˆ ëŒ€ê¸°
        )
        
        if not messages:
            return
        
        # ë©”ì‹œì§€ ì²˜ë¦¬
        stream_messages = messages[0][1]  # [stream_name, messages]
        processing_tasks = []
        
        for message_id, fields in stream_messages:
            task = self._process_single_event(message_id, fields)
            processing_tasks.append(task)
        
        # ë³‘ë ¬ ì²˜ë¦¬
        if processing_tasks:
            results = await asyncio.gather(*processing_tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì²˜ë¦¬
            successful_ids = []
            failed_ids = []
            
            for i, result in enumerate(results):
                message_id = stream_messages[i][0]
                
                if isinstance(result, Exception):
                    logging.error(f"Failed to process message {message_id}: {result}")
                    failed_ids.append(message_id)
                else:
                    successful_ids.append(message_id)
            
            # ì„±ê³µí•œ ë©”ì‹œì§€ ACK
            if successful_ids:
                await self.redis.xack(self.stream_name, self.consumer_group, *successful_ids)
                self.processed_count += len(successful_ids)
            
            # ì‹¤íŒ¨í•œ ë©”ì‹œì§€ëŠ” ì¬ì‹œë„ ë˜ëŠ” DLQë¡œ ì´ë™
            for message_id in failed_ids:
                await self._handle_failed_message(message_id)
    
    async def _process_single_event(self, message_id: str, fields: Dict[bytes, bytes]):
        """ë‹¨ì¼ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            # í•„ë“œ ë””ì½”ë”©
            decoded_fields = {
                k.decode('utf-8'): v.decode('utf-8') 
                for k, v in fields.items()
            }
            
            # ì´ë²¤íŠ¸ ë°ì´í„° íŒŒì‹±
            event_data = json.loads(decoded_fields.get('data', '{}'))
            event_type = decoded_fields.get('type', 'unknown')
            timestamp = datetime.fromisoformat(
                decoded_fields.get('timestamp', datetime.utcnow().isoformat())
            )
            
            # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì²˜ë¦¬
            if event_type == 'metric_point':
                await self._process_metric_point(event_data, timestamp)
            elif event_type == 'user_action':
                await self._process_user_action(event_data, timestamp)
            elif event_type == 'system_event':
                await self._process_system_event(event_data, timestamp)
            else:
                logging.warning(f"Unknown event type: {event_type}")
            
            # ì´ìƒ íƒì§€ ì‹¤í–‰
            await self._check_anomalies(event_data, event_type, timestamp)
            
        except Exception as e:
            logging.error(f"Error processing event {message_id}: {e}")
            raise
    
    async def _process_metric_point(self, data: Dict[str, Any], timestamp: datetime):
        """ë©”íŠ¸ë¦­ í¬ì¸íŠ¸ ì²˜ë¦¬"""
        metric_name = data.get('metric_name')
        value = data.get('value')
        dimensions = data.get('dimensions', {})
        
        if not metric_name or value is None:
            raise ValueError("Invalid metric point data")
        
        # ì‹¤ì‹œê°„ ì§‘ê³„ ì—…ë°ì´íŠ¸
        await self.aggregation_service.update_real_time_aggregation(
            metric_name=metric_name,
            value=value,
            dimensions=dimensions,
            timestamp=timestamp
        )
        
        # ë¶„ë‹¨ìœ„ ì§‘ê³„ ì—…ë°ì´íŠ¸
        await self.aggregation_service.update_minute_aggregation(
            metric_name=metric_name,
            value=value,
            dimensions=dimensions,
            timestamp=timestamp
        )
    
    async def _process_user_action(self, data: Dict[str, Any], timestamp: datetime):
        """ì‚¬ìš©ì ì•¡ì…˜ ì²˜ë¦¬"""
        user_id = data.get('user_id')
        action_type = data.get('action_type')
        target = data.get('target')
        
        # ì‚¬ìš©ì í™œë™ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        await self.aggregation_service.update_user_activity_metrics(
            user_id=user_id,
            action_type=action_type,
            target=target,
            timestamp=timestamp
        )
        
        # ì‹¤ì‹œê°„ ì‚¬ìš©ì ì¹´ìš´í„° ì—…ë°ì´íŠ¸
        await self._update_active_users(user_id, timestamp)
    
    async def _process_system_event(self, data: Dict[str, Any], timestamp: datetime):
        """ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        event_type = data.get('event_type')
        severity = data.get('severity', 'info')
        details = data.get('details', {})
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        await self.aggregation_service.update_system_metrics(
            event_type=event_type,
            severity=severity,
            details=details,
            timestamp=timestamp
        )
        
        # ì‹¬ê°í•œ ì´ë²¤íŠ¸ëŠ” ì¦‰ì‹œ ì•Œë¦¼
        if severity in ['critical', 'error']:
            await self._trigger_alert(event_type, details, timestamp)
    
    async def _check_anomalies(self, data: Dict[str, Any], event_type: str, timestamp: datetime):
        """ì´ìƒ íƒì§€ ì‹¤í–‰"""
        try:
            if event_type == 'metric_point':
                metric_name = data.get('metric_name')
                value = data.get('value')
                
                is_anomaly = await self.anomaly_detector.detect_anomaly(
                    metric_name=metric_name,
                    value=value,
                    timestamp=timestamp
                )
                
                if is_anomaly:
                    await self._handle_anomaly(metric_name, value, timestamp)
                    
        except Exception as e:
            logging.error(f"Anomaly detection error: {e}")
    
    async def _update_active_users(self, user_id: str, timestamp: datetime):
        """í™œì„± ì‚¬ìš©ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        # ìµœê·¼ 5ë¶„ê°„ í™œì„± ì‚¬ìš©ì ì¶”ì 
        active_users_key = f"active_users:{timestamp.strftime('%Y%m%d%H%M')}"
        
        await self.redis.sadd(active_users_key, user_id)
        await self.redis.expire(active_users_key, 300)  # 5ë¶„ TTL
        
        # ì‹¤ì‹œê°„ í™œì„± ì‚¬ìš©ì ìˆ˜ ì—…ë°ì´íŠ¸
        active_count = await self.redis.scard(active_users_key)
        await self.aggregation_service.update_real_time_metric(
            "active_users_count", active_count, timestamp
        )
    
    async def _trigger_alert(self, event_type: str, details: Dict[str, Any], timestamp: datetime):
        """ì•Œë¦¼ íŠ¸ë¦¬ê±°"""
        alert_data = {
            "type": "system_alert",
            "event_type": event_type,
            "details": details,
            "timestamp": timestamp.isoformat(),
            "severity": "high"
        }
        
        # ì•Œë¦¼ ìŠ¤íŠ¸ë¦¼ì— ì¶”ê°€
        await self.redis.xadd("alerts_stream", alert_data)
    
    async def _handle_anomaly(self, metric_name: str, value: float, timestamp: datetime):
        """ì´ìƒì¹˜ ì²˜ë¦¬"""
        anomaly_data = {
            "type": "anomaly_detected",
            "metric_name": metric_name,
            "value": value,
            "timestamp": timestamp.isoformat(),
            "severity": "medium"
        }
        
        # ì´ìƒì¹˜ ë¡œê·¸ ê¸°ë¡
        await self.redis.xadd("anomalies_stream", anomaly_data)
        
        logging.warning(f"Anomaly detected: {metric_name} = {value} at {timestamp}")
    
    async def _handle_failed_message(self, message_id: str):
        """ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ì²˜ë¦¬"""
        # ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
        retry_key = f"retry_count:{message_id}"
        retry_count = await self.redis.get(retry_key)
        retry_count = int(retry_count) if retry_count else 0
        
        if retry_count < self.max_retries:
            # ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€
            await self.redis.setex(retry_key, 3600, retry_count + 1)
            logging.info(f"Retrying message {message_id} (attempt {retry_count + 1})")
        else:
            # ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ - DLQë¡œ ì´ë™
            await self._move_to_dlq(message_id)
            await self.redis.xack(self.stream_name, self.consumer_group, message_id)
            logging.error(f"Message {message_id} moved to DLQ after {self.max_retries} attempts")
    
    async def _move_to_dlq(self, message_id: str):
        """ë°ë“œ ë ˆí„° íë¡œ ë©”ì‹œì§€ ì´ë™"""
        dlq_data = {
            "original_message_id": message_id,
            "failed_at": datetime.utcnow().isoformat(),
            "processor": self.consumer_name
        }
        
        await self.redis.xadd("failed_messages_stream", dlq_data)
    
    async def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ì¡°íšŒ"""
        uptime = datetime.utcnow() - self.last_process_time
        
        return {
            "processed_count": self.processed_count,
            "error_count": self.error_count,
            "error_rate": self.error_count / max(self.processed_count, 1),
            "uptime_seconds": uptime.total_seconds(),
            "processing_rate": self.processed_count / max(uptime.total_seconds(), 1)
        }
```

### ì‹¤ì‹œê°„ ì§‘ê³„ ì„œë¹„ìŠ¤
```python
# app/domains/statistics/services/aggregation_service.py

import asyncio
import json
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from collections import defaultdict
import redis.asyncio as redis
import numpy as np

class AggregationService:
    """ì‹¤ì‹œê°„ ì§‘ê³„ ì„œë¹„ìŠ¤"""
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        
        # ì§‘ê³„ ì„¤ì •
        self.aggregation_windows = {
            "1m": 60,        # 1ë¶„
            "5m": 300,       # 5ë¶„
            "15m": 900,      # 15ë¶„
            "1h": 3600,      # 1ì‹œê°„
            "1d": 86400      # 1ì¼
        }
        
        # ë°ì´í„° ë³´ì¡´ ê¸°ê°„
        self.retention_periods = {
            "1m": 3600,      # 1ì‹œê°„
            "5m": 86400,     # 1ì¼
            "15m": 604800,   # 1ì£¼
            "1h": 2592000,   # 30ì¼
            "1d": 31536000   # 1ë…„
        }
    
    async def update_real_time_aggregation(
        self, 
        metric_name: str, 
        value: float, 
        dimensions: Dict[str, str] = None,
        timestamp: datetime = None
    ):
        """ì‹¤ì‹œê°„ ì§‘ê³„ ì—…ë°ì´íŠ¸"""
        if timestamp is None:
            timestamp = datetime.utcnow()
        
        dimensions = dimensions or {}
        
        # ëª¨ë“  ì§‘ê³„ ìœˆë„ìš°ì— ëŒ€í•´ ì—…ë°ì´íŠ¸
        update_tasks = []
        for window_name, window_seconds in self.aggregation_windows.items():
            task = self._update_window_aggregation(
                metric_name, value, dimensions, timestamp, window_name, window_seconds
            )
            update_tasks.append(task)
        
        await asyncio.gather(*update_tasks)
    
    async def _update_window_aggregation(
        self,
        metric_name: str,
        value: float,
        dimensions: Dict[str, str],
        timestamp: datetime,
        window_name: str,
        window_seconds: int
    ):
        """íŠ¹ì • ìœˆë„ìš° ì§‘ê³„ ì—…ë°ì´íŠ¸"""
        
        # ìœˆë„ìš° ì‹œì‘ ì‹œê°„ ê³„ì‚°
        window_start = self._get_window_start(timestamp, window_seconds)
        
        # ê¸°ë³¸ í‚¤ ìƒì„±
        base_key = f"agg:{metric_name}:{window_name}:{window_start.timestamp()}"
        
        # ì°¨ì›ì´ ìˆëŠ” ê²½ìš° ì°¨ì›ë³„ í‚¤ë„ ìƒì„±
        keys_to_update = [base_key]
        if dimensions:
            for dim_key, dim_value in dimensions.items():
                dim_key_name = f"{base_key}:{dim_key}:{dim_value}"
                keys_to_update.append(dim_key_name)
        
        # Redis íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë°°ì¹˜ ì—…ë°ì´íŠ¸
        pipe = self.redis.pipeline()
        
        for key in keys_to_update:
            # ì¹´ìš´í„° ì¦ê°€
            pipe.hincrby(key, "count", 1)
            
            # í•©ê³„ ì—…ë°ì´íŠ¸
            pipe.hincrbyfloat(key, "sum", value)
            
            # ìµœëŒ€ê°’ ì—…ë°ì´íŠ¸
            pipe.eval("""
                local current_max = redis.call('hget', KEYS[1], 'max')
                if current_max == false or tonumber(ARGV[1]) > tonumber(current_max) then
                    redis.call('hset', KEYS[1], 'max', ARGV[1])
                end
            """, 1, key, value)
            
            # ìµœì†Œê°’ ì—…ë°ì´íŠ¸
            pipe.eval("""
                local current_min = redis.call('hget', KEYS[1], 'min')
                if current_min == false or tonumber(ARGV[1]) < tonumber(current_min) then
                    redis.call('hset', KEYS[1], 'min', ARGV[1])
                end
            """, 1, key, value)
            
            # TTL ì„¤ì •
            pipe.expire(key, self.retention_periods.get(window_name, 3600))
        
        await pipe.execute()
        
        # ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ê°’ ì €ì¥ (ìƒ˜í”Œë§)
        await self._update_percentiles(base_key, value, window_name)
    
    async def _update_percentiles(self, base_key: str, value: float, window_name: str):
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ê°’ ì €ì¥"""
        percentile_key = f"{base_key}:values"
        
        # HyperLogLogë¥¼ ì‚¬ìš©í•œ ê·¼ì‚¬ ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
        # ë˜ëŠ” ìƒ˜í”Œë§ì„ í†µí•œ ì •í™•í•œ ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
        
        # ìƒ˜í”Œë§ í™•ë¥  (ë©”ëª¨ë¦¬ ì ˆì•½)
        sampling_rate = 0.1 if window_name in ["1m", "5m"] else 0.01
        
        if np.random.random() < sampling_rate:
            await self.redis.lpush(percentile_key, value)
            await self.redis.ltrim(percentile_key, 0, 999)  # ìµœëŒ€ 1000ê°œ ê°’ ìœ ì§€
            await self.redis.expire(percentile_key, self.retention_periods.get(window_name, 3600))
    
    async def get_real_time_aggregation(
        self,
        metric_name: str,
        window_name: str = "5m",
        dimensions: Dict[str, str] = None
    ) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ì§‘ê³„ ë°ì´í„° ì¡°íšŒ"""
        
        current_time = datetime.utcnow()
        window_seconds = self.aggregation_windows.get(window_name, 300)
        window_start = self._get_window_start(current_time, window_seconds)
        
        # í‚¤ ìƒì„±
        base_key = f"agg:{metric_name}:{window_name}:{window_start.timestamp()}"
        
        if dimensions:
            # íŠ¹ì • ì°¨ì› ì¡°íšŒ
            dimension_results = {}
            for dim_key, dim_value in dimensions.items():
                dim_key_name = f"{base_key}:{dim_key}:{dim_value}"
                dim_data = await self.redis.hgetall(dim_key_name)
                
                if dim_data:
                    dimension_results[f"{dim_key}:{dim_value}"] = self._parse_aggregation_data(dim_data)
            
            return dimension_results
        else:
            # ì „ì²´ ì§‘ê³„ ì¡°íšŒ
            agg_data = await self.redis.hgetall(base_key)
            
            if not agg_data:
                return self._empty_aggregation_result()
            
            result = self._parse_aggregation_data(agg_data)
            
            # ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
            percentiles = await self._calculate_percentiles(f"{base_key}:values")
            result.update(percentiles)
            
            return result
    
    async def get_time_series_data(
        self,
        metric_name: str,
        start_time: datetime,
        end_time: datetime,
        window_name: str = "1h",
        dimensions: Dict[str, str] = None
    ) -> List[Dict[str, Any]]:
        """ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ"""
        
        window_seconds = self.aggregation_windows.get(window_name, 3600)
        time_series = []
        
        # ì‹œê°„ ë²”ìœ„ ë‚´ì˜ ëª¨ë“  ìœˆë„ìš° ìˆœíšŒ
        current_time = self._get_window_start(start_time, window_seconds)
        
        while current_time <= end_time:
            base_key = f"agg:{metric_name}:{window_name}:{current_time.timestamp()}"
            
            if dimensions:
                # ì°¨ì›ë³„ ë°ì´í„° ì¡°íšŒ
                for dim_key, dim_value in dimensions.items():
                    dim_key_name = f"{base_key}:{dim_key}:{dim_value}"
                    agg_data = await self.redis.hgetall(dim_key_name)
                    
                    if agg_data:
                        point = self._parse_aggregation_data(agg_data)
                        point.update({
                            "timestamp": current_time,
                            "dimension": f"{dim_key}:{dim_value}"
                        })
                        time_series.append(point)
            else:
                # ì „ì²´ ì§‘ê³„ ë°ì´í„° ì¡°íšŒ
                agg_data = await self.redis.hgetall(base_key)
                
                if agg_data:
                    point = self._parse_aggregation_data(agg_data)
                    point["timestamp"] = current_time
                    time_series.append(point)
                else:
                    # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì±„ì›€
                    time_series.append({
                        "timestamp": current_time,
                        "count": 0,
                        "sum": 0,
                        "avg": 0,
                        "min": 0,
                        "max": 0
                    })
            
            # ë‹¤ìŒ ìœˆë„ìš°ë¡œ ì´ë™
            current_time += timedelta(seconds=window_seconds)
        
        return time_series
    
    def _get_window_start(self, timestamp: datetime, window_seconds: int) -> datetime:
        """ìœˆë„ìš° ì‹œì‘ ì‹œê°„ ê³„ì‚°"""
        epoch = datetime(1970, 1, 1)
        seconds_since_epoch = (timestamp - epoch).total_seconds()
        window_start_seconds = (seconds_since_epoch // window_seconds) * window_seconds
        return datetime.utcfromtimestamp(window_start_seconds)
    
    def _parse_aggregation_data(self, agg_data: Dict[bytes, bytes]) -> Dict[str, Any]:
        """ì§‘ê³„ ë°ì´í„° íŒŒì‹±"""
        count = int(agg_data.get(b'count', 0))
        sum_val = float(agg_data.get(b'sum', 0))
        min_val = float(agg_data.get(b'min', 0))
        max_val = float(agg_data.get(b'max', 0))
        
        avg_val = sum_val / count if count > 0 else 0
        
        return {
            "count": count,
            "sum": sum_val,
            "avg": avg_val,
            "min": min_val,
            "max": max_val
        }
    
    def _empty_aggregation_result(self) -> Dict[str, Any]:
        """ë¹ˆ ì§‘ê³„ ê²°ê³¼"""
        return {
            "count": 0,
            "sum": 0,
            "avg": 0,
            "min": 0,
            "max": 0
        }
    
    async def _calculate_percentiles(self, values_key: str) -> Dict[str, float]:
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
        values = await self.redis.lrange(values_key, 0, -1)
        
        if not values:
            return {"p50": 0, "p90": 0, "p95": 0, "p99": 0}
        
        # bytesë¥¼ floatë¡œ ë³€í™˜
        float_values = [float(v) for v in values]
        float_values.sort()
        
        percentiles = {}
        for p in [50, 90, 95, 99]:
            idx = int(len(float_values) * (p / 100.0))
            if idx >= len(float_values):
                idx = len(float_values) - 1
            percentiles[f"p{p}"] = float_values[idx]
        
        return percentiles
```

## ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„ ë° ì˜ˆì¸¡

### ì‹œê³„ì—´ ë¶„ì„ ì„œë¹„ìŠ¤
```python
# app/domains/statistics/ml/forecasting.py

import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime, timedelta
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
import logging

# Prophetê³¼ ARIMAëŠ” ì„ íƒì  import (ê°€ë²¼ìš´ ëŒ€ì•ˆ ì œê³µ)
try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    logging.warning("Prophet not available, using simplified forecasting")

try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.seasonal import seasonal_decompose
    ARIMA_AVAILABLE = True
except ImportError:
    ARIMA_AVAILABLE = False
    logging.warning("ARIMA not available, using linear models")

class ForecastingService:
    """ì‹œê³„ì—´ ì˜ˆì¸¡ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.model_performance = {}
        
        # ì˜ˆì¸¡ ì„¤ì •
        self.min_data_points = 30
        self.confidence_levels = [0.8, 0.9, 0.95]
        self.default_confidence = 0.95
    
    async def generate_forecast(
        self,
        metric_name: str,
        historical_data: List[Dict[str, Any]],
        horizon_days: int = 7,
        model_type: str = "auto",
        confidence_level: float = 0.95
    ) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ì˜ˆì¸¡ ìƒì„±"""
        
        if len(historical_data) < self.min_data_points:
            raise ValueError(f"Insufficient data points. Need at least {self.min_data_points}, got {len(historical_data)}")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        df = self._prepare_data(historical_data)
        
        # ëª¨ë¸ ì„ íƒ
        if model_type == "auto":
            model_type = self._select_best_model(df)
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        if model_type == "prophet" and PROPHET_AVAILABLE:
            forecast_result = await self._prophet_forecast(df, horizon_days, confidence_level)
        elif model_type == "arima" and ARIMA_AVAILABLE:
            forecast_result = await self._arima_forecast(df, horizon_days, confidence_level)
        elif model_type == "linear":
            forecast_result = await self._linear_forecast(df, horizon_days, confidence_level)
        else:
            # í´ë°±: ë‹¨ìˆœ ì„ í˜• ëª¨ë¸
            forecast_result = await self._simple_linear_forecast(df, horizon_days, confidence_level)
        
        # ê²°ê³¼ í›„ì²˜ë¦¬
        forecast_result.update({
            "metric_name": metric_name,
            "model_type": model_type,
            "data_points_used": len(df),
            "forecast_horizon": horizon_days,
            "confidence_level": confidence_level,
            "generated_at": datetime.utcnow()
        })
        
        return forecast_result
    
    def _prepare_data(self, historical_data: List[Dict[str, Any]]) -> pd.DataFrame:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(historical_data)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
        if 'timestamp' in df.columns:
            df['ds'] = pd.to_datetime(df['timestamp'])
        elif 'date' in df.columns:
            df['ds'] = pd.to_datetime(df['date'])
        else:
            raise ValueError("No timestamp column found")
        
        # ê°’ ì»¬ëŸ¼ ì„¤ì •
        if 'value' in df.columns:
            df['y'] = df['value']
        elif 'avg' in df.columns:
            df['y'] = df['avg']
        elif 'sum' in df.columns:
            df['y'] = df['sum']
        else:
            raise ValueError("No value column found")
        
        # ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        df = df.sort_values('ds').drop_duplicates(subset=['ds']).reset_index(drop=True)
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df['y'] = df['y'].fillna(method='ffill').fillna(0)
        
        return df[['ds', 'y']]
    
    def _select_best_model(self, df: pd.DataFrame) -> str:
        """ìµœì  ëª¨ë¸ ìë™ ì„ íƒ"""
        
        # ë°ì´í„° íŠ¹ì„± ë¶„ì„
        data_length = len(df)
        variance = df['y'].var()
        trend_strength = self._calculate_trend_strength(df)
        seasonality_strength = self._calculate_seasonality_strength(df)
        
        # ëª¨ë¸ ì„ íƒ ë¡œì§
        if data_length >= 365 and seasonality_strength > 0.3 and PROPHET_AVAILABLE:
            return "prophet"
        elif data_length >= 100 and trend_strength > 0.2 and ARIMA_AVAILABLE:
            return "arima"
        elif trend_strength > 0.1:
            return "linear"
        else:
            return "simple_linear"
    
    def _calculate_trend_strength(self, df: pd.DataFrame) -> float:
        """íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚°"""
        if len(df) < 10:
            return 0
        
        x = np.arange(len(df))
        y = df['y'].values
        
        correlation = np.corrcoef(x, y)[0, 1]
        return abs(correlation) if not np.isnan(correlation) else 0
    
    def _calculate_seasonality_strength(self, df: pd.DataFrame) -> float:
        """ê³„ì ˆì„± ê°•ë„ ê³„ì‚°"""
        if len(df) < 30:
            return 0
        
        try:
            # ì£¼ê°„ ê³„ì ˆì„± í™•ì¸
            weekly_pattern = df.set_index('ds')['y'].resample('D').mean()
            if len(weekly_pattern) >= 14:
                weekly_corr = []
                for i in range(7):
                    if len(weekly_pattern) > i + 7:
                        week1 = weekly_pattern.iloc[i::7].values
                        week2 = weekly_pattern.iloc[i+7::7].values
                        min_len = min(len(week1), len(week2))
                        if min_len > 1:
                            corr = np.corrcoef(week1[:min_len], week2[:min_len])[0, 1]
                            if not np.isnan(corr):
                                weekly_corr.append(abs(corr))
                
                if weekly_corr:
                    return np.mean(weekly_corr)
        except:
            pass
        
        return 0
    
    async def _prophet_forecast(
        self, df: pd.DataFrame, horizon_days: int, confidence_level: float
    ) -> Dict[str, Any]:
        """Prophet ëª¨ë¸ ì˜ˆì¸¡"""
        
        # Prophet ëª¨ë¸ ì´ˆê¸°í™”
        model = Prophet(
            yearly_seasonality=len(df) >= 365,
            weekly_seasonality=len(df) >= 14,
            daily_seasonality=False,
            interval_width=confidence_level
        )
        
        # ëª¨ë¸ í•™ìŠµ
        model.fit(df)
        
        # ë¯¸ë˜ ë‚ ì§œ ìƒì„±
        future = model.make_future_dataframe(periods=horizon_days)
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        forecast = model.predict(future)
        
        # ê²°ê³¼ ì¶”ì¶œ
        forecast_data = forecast.tail(horizon_days)
        
        forecast_values = forecast_data['yhat'].tolist()
        forecast_timestamps = forecast_data['ds'].dt.to_pydatetime().tolist()
        
        confidence_intervals = []
        for _, row in forecast_data.iterrows():
            confidence_intervals.append({
                "lower": row['yhat_lower'],
                "upper": row['yhat_upper']
            })
        
        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        historical_predictions = forecast.iloc[:-horizon_days]['yhat']
        actual_values = df['y']
        mae = mean_absolute_error(actual_values, historical_predictions)
        rmse = np.sqrt(mean_squared_error(actual_values, historical_predictions))
        
        accuracy = max(0, 1 - (mae / (actual_values.mean() + 1e-8)))
        
        return {
            "forecast_values": forecast_values,
            "forecast_timestamps": forecast_timestamps,
            "confidence_intervals": confidence_intervals,
            "model_accuracy": accuracy,
            "mae": mae,
            "rmse": rmse,
            "model_details": {
                "yearly_seasonality": model.yearly_seasonality,
                "weekly_seasonality": model.weekly_seasonality
            }
        }
    
    async def _arima_forecast(
        self, df: pd.DataFrame, horizon_days: int, confidence_level: float
    ) -> Dict[str, Any]:
        """ARIMA ëª¨ë¸ ì˜ˆì¸¡"""
        
        # ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
        ts = df.set_index('ds')['y']
        
        # ìë™ ARIMA ì°¨ìˆ˜ ì„ íƒ (ê°„ë‹¨í•œ ë²„ì „)
        order = self._select_arima_order(ts)
        
        # ARIMA ëª¨ë¸ í•™ìŠµ
        model = ARIMA(ts, order=order)
        fitted_model = model.fit()
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        forecast_result = fitted_model.forecast(steps=horizon_days, alpha=1-confidence_level)
        forecast_values = forecast_result.tolist()
        
        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
        conf_int = fitted_model.get_forecast(steps=horizon_days, alpha=1-confidence_level).conf_int()
        confidence_intervals = []
        for i in range(len(conf_int)):
            confidence_intervals.append({
                "lower": conf_int.iloc[i, 0],
                "upper": conf_int.iloc[i, 1]
            })
        
        # ë¯¸ë˜ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        last_date = df['ds'].iloc[-1]
        forecast_timestamps = [
            last_date + timedelta(days=i+1) for i in range(horizon_days)
        ]
        
        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        fitted_values = fitted_model.fittedvalues
        actual_values = ts
        mae = mean_absolute_error(actual_values, fitted_values)
        accuracy = max(0, 1 - (mae / (actual_values.mean() + 1e-8)))
        
        return {
            "forecast_values": forecast_values,
            "forecast_timestamps": forecast_timestamps,
            "confidence_intervals": confidence_intervals,
            "model_accuracy": accuracy,
            "model_details": {
                "arima_order": order,
                "aic": fitted_model.aic
            }
        }
    
    async def _linear_forecast(
        self, df: pd.DataFrame, horizon_days: int, confidence_level: float
    ) -> Dict[str, Any]:
        """ì„ í˜• íšŒê·€ ì˜ˆì¸¡"""
        
        # íŠ¹ì„± ìƒì„±
        df_copy = df.copy()
        df_copy['timestamp_numeric'] = pd.to_datetime(df_copy['ds']).astype(int) / 10**9
        
        # ì¶”ê°€ íŠ¹ì„± (ìš”ì¼, ì›” ë“±)
        df_copy['day_of_week'] = pd.to_datetime(df_copy['ds']).dt.dayofweek
        df_copy['month'] = pd.to_datetime(df_copy['ds']).dt.month
        df_copy['day_of_year'] = pd.to_datetime(df_copy['ds']).dt.dayofyear
        
        # íŠ¹ì„± í–‰ë ¬ êµ¬ì„±
        feature_cols = ['timestamp_numeric', 'day_of_week', 'month']
        if len(df_copy) >= 365:
            feature_cols.append('day_of_year')
        
        X = df_copy[feature_cols].values
        y = df_copy['y'].values
        
        # ëª¨ë¸ í•™ìŠµ
        model = LinearRegression()
        model.fit(X, y)
        
        # ì˜ˆì¸¡ìš© ë¯¸ë˜ ë°ì´í„° ìƒì„±
        last_date = pd.to_datetime(df['ds'].iloc[-1])
        future_dates = [last_date + timedelta(days=i+1) for i in range(horizon_days)]
        
        future_features = []
        for date in future_dates:
            features = [
                date.timestamp(),
                date.weekday(),
                date.month
            ]
            if 'day_of_year' in feature_cols:
                features.append(date.timetuple().tm_yday)
            future_features.append(features)
        
        future_X = np.array(future_features)
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        forecast_values = model.predict(future_X).tolist()
        
        # ì‹ ë¢°êµ¬ê°„ ê·¼ì‚¬ ê³„ì‚°
        residuals = y - model.predict(X)
        residual_std = np.std(residuals)
        
        # ì •ê·œë¶„í¬ ê°€ì •í•˜ì— ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
        z_score = 1.96 if confidence_level == 0.95 else 1.64  # ê°„ë‹¨í•œ ê·¼ì‚¬
        
        confidence_intervals = []
        for forecast_val in forecast_values:
            margin = z_score * residual_std
            confidence_intervals.append({
                "lower": forecast_val - margin,
                "upper": forecast_val + margin
            })
        
        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        predictions = model.predict(X)
        mae = mean_absolute_error(y, predictions)
        accuracy = max(0, 1 - (mae / (y.mean() + 1e-8)))
        
        return {
            "forecast_values": forecast_values,
            "forecast_timestamps": future_dates,
            "confidence_intervals": confidence_intervals,
            "model_accuracy": accuracy,
            "model_details": {
                "features_used": feature_cols,
                "r2_score": model.score(X, y)
            }
        }
    
    async def _simple_linear_forecast(
        self, df: pd.DataFrame, horizon_days: int, confidence_level: float
    ) -> Dict[str, Any]:
        """ë‹¨ìˆœ ì„ í˜• íŠ¸ë Œë“œ ì˜ˆì¸¡ (í´ë°±)"""
        
        # ì‹œê°„ì„ ìˆ«ìë¡œ ë³€í™˜
        x = np.arange(len(df))
        y = df['y'].values
        
        # ì„ í˜• íšŒê·€
        coeffs = np.polyfit(x, y, 1)
        slope, intercept = coeffs
        
        # ë¯¸ë˜ ì˜ˆì¸¡
        future_x = np.arange(len(df), len(df) + horizon_days)
        forecast_values = (slope * future_x + intercept).tolist()
        
        # ë¯¸ë˜ íƒ€ì„ìŠ¤íƒ¬í”„
        last_date = pd.to_datetime(df['ds'].iloc[-1])
        forecast_timestamps = [
            last_date + timedelta(days=i+1) for i in range(horizon_days)
        ]
        
        # ê°„ë‹¨í•œ ì‹ ë¢°êµ¬ê°„ (ì”ì°¨ ê¸°ë°˜)
        predictions = slope * x + intercept
        residuals = y - predictions
        residual_std = np.std(residuals)
        
        confidence_intervals = []
        for forecast_val in forecast_values:
            margin = 1.96 * residual_std  # 95% ì‹ ë¢°êµ¬ê°„
            confidence_intervals.append({
                "lower": forecast_val - margin,
                "upper": forecast_val + margin
            })
        
        # ì •í™•ë„ ê³„ì‚°
        mae = np.mean(np.abs(residuals))
        accuracy = max(0, 1 - (mae / (y.mean() + 1e-8)))
        
        return {
            "forecast_values": forecast_values,
            "forecast_timestamps": forecast_timestamps,
            "confidence_intervals": confidence_intervals,
            "model_accuracy": accuracy,
            "model_details": {
                "slope": slope,
                "intercept": intercept,
                "trend_direction": "up" if slope > 0 else "down" if slope < 0 else "stable"
            }
        }
    
    def _select_arima_order(self, ts: pd.Series) -> Tuple[int, int, int]:
        """ARIMA ì°¨ìˆ˜ ìë™ ì„ íƒ (ê°„ë‹¨í•œ ë²„ì „)"""
        # ì‹¤ì œë¡œëŠ” AIC/BIC ê¸°ë°˜ ê·¸ë¦¬ë“œ ì„œì¹˜ë¥¼ í•´ì•¼ í•˜ì§€ë§Œ, 
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
        
        # ì°¨ë¶„ í•„ìš”ì„± í™•ì¸ (ê°„ë‹¨í•œ ë°©ë²•)
        diff_needed = 0
        current_series = ts
        
        # ìµœëŒ€ 2ì°¨ ì°¨ë¶„ê¹Œì§€ í™•ì¸
        for d in range(3):
            # ë‹¨ìœ„ê·¼ ê²€ì • ëŒ€ì‹  ë¶„ì‚° ê°ì†Œ í™•ì¸
            if d == 0:
                if current_series.var() < ts.var() * 1.1:  # ê±°ì˜ ì°¨ì´ ì—†ìœ¼ë©´ ì°¨ë¶„ ë¶ˆí•„ìš”
                    diff_needed = d
                    break
            else:
                diff_series = current_series.diff().dropna()
                if len(diff_series) < 10:
                    break
                if diff_series.var() < current_series.var():
                    current_series = diff_series
                    diff_needed = d
                else:
                    break
        
        # AR, MA ì°¨ìˆ˜ëŠ” ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ê²°ì •
        data_length = len(ts)
        if data_length < 50:
            p, q = 1, 1
        elif data_length < 100:
            p, q = 2, 1
        else:
            p, q = 2, 2
        
        return (p, diff_needed, q)
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ì§€í‘œ

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ
```python
# app/domains/statistics/monitoring/metrics_collector.py

from prometheus_client import Counter, Histogram, Gauge, Summary
import time
from functools import wraps
from typing import Dict, Any
import asyncio

# Statistics ë„ë©”ì¸ ë©”íŠ¸ë¦­ ì •ì˜
statistics_api_requests_total = Counter(
    'statistics_api_requests_total',
    'Total statistics API requests',
    ['method', 'endpoint', 'status']
)

statistics_processing_duration = Histogram(
    'statistics_processing_duration_seconds',
    'Statistics processing duration',
    ['operation_type']
)

real_time_events_processed = Counter(
    'real_time_events_processed_total',
    'Total real-time events processed',
    ['event_type', 'status']
)

aggregation_lag_seconds = Gauge(
    'aggregation_lag_seconds',
    'Aggregation processing lag in seconds',
    ['aggregation_level']
)

active_metrics_count = Gauge(
    'active_metrics_count',
    'Number of active metrics being tracked'
)

forecast_accuracy = Gauge(
    'forecast_accuracy_score',
    'Forecast model accuracy score',
    ['metric_name', 'model_type']
)

def monitor_statistics_operation(operation_type: str):
    """í†µê³„ ì‘ì—… ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            status = "success"
            
            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                status = "error"
                raise
            finally:
                duration = time.time() - start_time
                statistics_processing_duration.labels(
                    operation_type=operation_type
                ).observe(duration)
        
        return wrapper
    return decorator

class MetricsCollector:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.collection_interval = 60  # 1ë¶„ë§ˆë‹¤ ìˆ˜ì§‘
        self.is_running = False
    
    async def start_collection(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘"""
        self.is_running = True
        
        while self.is_running:
            try:
                await self._collect_metrics()
                await asyncio.sleep(self.collection_interval)
            except Exception as e:
                print(f"Metrics collection error: {e}")
                await asyncio.sleep(5)
    
    async def _collect_metrics(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤í–‰"""
        # í™œì„± ë©”íŠ¸ë¦­ ìˆ˜ ì—…ë°ì´íŠ¸
        # active_count = await self._count_active_metrics()
        # active_metrics_count.set(active_count)
        
        # ì§‘ê³„ ì§€ì—° ì‹œê°„ ì¸¡ì •
        # lag_metrics = await self._measure_aggregation_lag()
        # for level, lag in lag_metrics.items():
        #     aggregation_lag_seconds.labels(aggregation_level=level).set(lag)
        
        pass  # ì‹¤ì œ êµ¬í˜„ì€ ê° ì„œë¹„ìŠ¤ì™€ ì—°ë™
    
    def stop_collection(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ì§€"""
        self.is_running = False
```

## ğŸ”„ ì—…ë°ì´íŠ¸ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ì‚¬í•­ | ì‘ì„±ì |
|------|------|----------|--------|
| 1.0.0 | 2025-08-14 | ì´ˆê¸° Statistics ë„ë©”ì¸ êµ¬í˜„ ë¬¸ì„œ ì‘ì„± | Backend Team |

---

*Statistics ë„ë©”ì¸ì€ Korea Public Data í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ë¶„ì„ ì—”ì§„ìœ¼ë¡œ, ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ì™€ AI ê¸°ë°˜ ì˜ˆì¸¡ì„ í†µí•´ ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•˜ê² ìŠµë‹ˆë‹¤.*