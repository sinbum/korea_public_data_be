# ğŸ“š Content ë„ë©”ì¸ êµ¬í˜„ í˜„í™©

> Korea Public Data Backendì˜ Content ë„ë©”ì¸ ìƒì„¸ êµ¬í˜„ í˜„í™© ë° ê¸°ìˆ  ë¬¸ì„œ

## ğŸ“‹ ëª©ì°¨
- [ë„ë©”ì¸ ê°œìš”](#ë„ë©”ì¸-ê°œìš”)
- [ì•„í‚¤í…ì²˜ êµ¬ì¡°](#ì•„í‚¤í…ì²˜-êµ¬ì¡°)
- [ë°ì´í„° ëª¨ë¸](#ë°ì´í„°-ëª¨ë¸)
- [API ì—”ë“œí¬ì¸íŠ¸](#api-ì—”ë“œí¬ì¸íŠ¸)
- [AI ë¶„ë¥˜ ì‹œìŠ¤í…œ](#ai-ë¶„ë¥˜-ì‹œìŠ¤í…œ)
- [ê²€ìƒ‰ ì—”ì§„](#ê²€ìƒ‰-ì—”ì§„)
- [ì¶”ì²œ ì‹œìŠ¤í…œ](#ì¶”ì²œ-ì‹œìŠ¤í…œ)
- [í’ˆì§ˆ ê´€ë¦¬](#í’ˆì§ˆ-ê´€ë¦¬)
- [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)

## ğŸ¯ ë„ë©”ì¸ ê°œìš”

### Content ë„ë©”ì¸ì˜ ì—­í• 
Content ë„ë©”ì¸ì€ ê³µê³µë°ì´í„° ê´€ë ¨ ì½˜í…ì¸ (ê°€ì´ë“œ, ë§¤ë‰´ì–¼, ë‰´ìŠ¤, ë¶„ì„ ë¦¬í¬íŠ¸ ë“±)ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³ , AI ê¸°ë°˜ ìë™ ë¶„ë¥˜, ì§€ëŠ¥í˜• ê²€ìƒ‰, ê°œì¸í™” ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ë„ë©”ì¸ì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
1. **ì½˜í…ì¸  ê´€ë¦¬**: CRUD ë° ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ
2. **AI ë¶„ë¥˜ ì‹œìŠ¤í…œ**: NLP ê¸°ë°˜ ìë™ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
3. **ì§€ëŠ¥í˜• ê²€ìƒ‰**: ì „ë¬¸ ê²€ìƒ‰ ë° ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
4. **í’ˆì§ˆ í‰ê°€**: ë‹¤ì¤‘ ì§€í‘œ ê¸°ë°˜ ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€
5. **ì¶”ì²œ ì—”ì§„**: í˜‘ì—… í•„í„°ë§ ê¸°ë°˜ ê°œì¸í™” ì¶”ì²œ
6. **íƒœê·¸ ì‹œìŠ¤í…œ**: ìë™ íƒœê·¸ ì¶”ì¶œ ë° ê´€ë¦¬

### í˜„ì¬ êµ¬í˜„ ìƒíƒœ
- **ì™„ì„±ë„**: 78% ğŸŸ¡
- **í”„ë¡œë•ì…˜ ì¤€ë¹„ë„**: 85% ğŸŸ¡
- **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: 75% ğŸŸ¡
- **AI ëª¨ë¸ ì •í™•ë„**: 85% âœ…

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì¡°

### ë„ë©”ì¸ ê³„ì¸µ êµ¬ì¡°
```mermaid
graph TB
    A[API Layer] --> B[Service Layer]
    B --> C[Repository Layer]
    C --> D[MongoDB Database]
    
    B --> E[AI/ML Services]
    E --> F[BERT Classifier]
    E --> G[Tag Extractor]
    E --> H[Quality Assessor]
    
    B --> I[Search Engine]
    I --> J[Elasticsearch]
    I --> K[Vector Store]
    
    L[Background Tasks] --> B
    M[Content Pipeline] --> E
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
```

### íŒŒì¼ êµ¬ì¡°
```
app/domains/contents/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ content.py           # í•µì‹¬ ì½˜í…ì¸  ëª¨ë¸
â”‚   â”œâ”€â”€ category.py          # ì¹´í…Œê³ ë¦¬ ëª¨ë¸
â”‚   â”œâ”€â”€ tag.py              # íƒœê·¸ ëª¨ë¸
â”‚   â””â”€â”€ quality.py          # í’ˆì§ˆ í‰ê°€ ëª¨ë¸
â”œâ”€â”€ repositories/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ content_repository.py
â”‚   â”œâ”€â”€ category_repository.py
â”‚   â””â”€â”€ search_repository.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ content_service.py   # í•µì‹¬ ì½˜í…ì¸  ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ classification_service.py  # AI ë¶„ë¥˜ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ search_service.py    # ê²€ìƒ‰ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ recommendation_service.py  # ì¶”ì²œ ì„œë¹„ìŠ¤
â”‚   â””â”€â”€ quality_service.py   # í’ˆì§ˆ ê´€ë¦¬ ì„œë¹„ìŠ¤
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ content_router.py    # API ë¼ìš°í„°
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ request.py          # ìš”ì²­ ìŠ¤í‚¤ë§ˆ
â”‚   â””â”€â”€ response.py         # ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bert_classifier.py  # BERT ê¸°ë°˜ ë¶„ë¥˜ê¸°
â”‚   â”œâ”€â”€ tag_extractor.py    # íƒœê·¸ ì¶”ì¶œê¸°
â”‚   â””â”€â”€ quality_scorer.py   # í’ˆì§ˆ í‰ê°€ê¸°
â””â”€â”€ tasks/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ classification_tasks.py
    â””â”€â”€ recommendation_tasks.py
```

## ğŸ“Š ë°ì´í„° ëª¨ë¸

### Content í•µì‹¬ ëª¨ë¸
```python
# app/domains/contents/models/content.py

from datetime import datetime
from typing import Optional, List, Dict, Any, Union
from pydantic import BaseModel, Field, HttpUrl, validator
from bson import ObjectId
from enum import Enum

class ContentType(str, Enum):
    """ì½˜í…ì¸  íƒ€ì…"""
    GUIDE = "guide"           # ê°€ì´ë“œ/ë§¤ë‰´ì–¼
    NEWS = "news"             # ë‰´ìŠ¤/ê³µì§€ì‚¬í•­
    REPORT = "report"         # ë¶„ì„ ë¦¬í¬íŠ¸
    TUTORIAL = "tutorial"     # íŠœí† ë¦¬ì–¼
    FAQ = "faq"              # FAQ
    DOCUMENTATION = "documentation"  # ê¸°ìˆ  ë¬¸ì„œ
    CASE_STUDY = "case_study" # ì‚¬ë¡€ ì—°êµ¬
    POLICY = "policy"         # ì •ì±… ë¬¸ì„œ

class ContentStatus(str, Enum):
    """ì½˜í…ì¸  ìƒíƒœ"""
    DRAFT = "draft"           # ì´ˆì•ˆ
    REVIEW = "review"         # ê²€í†  ì¤‘
    PUBLISHED = "published"   # ê²Œì‹œë¨
    ARCHIVED = "archived"     # ë³´ê´€ë¨
    DELETED = "deleted"       # ì‚­ì œë¨

class ContentBase(BaseModel):
    """ì½˜í…ì¸  ê¸°ë³¸ ëª¨ë¸"""
    title: str = Field(..., min_length=5, max_length=200, description="ì½˜í…ì¸  ì œëª©")
    content: str = Field(..., min_length=50, description="ì½˜í…ì¸  ë³¸ë¬¸")
    content_type: ContentType = Field(..., description="ì½˜í…ì¸  íƒ€ì…")
    
    # ë©”íƒ€ë°ì´í„°
    summary: Optional[str] = Field(None, max_length=500, description="ì½˜í…ì¸  ìš”ì•½")
    excerpt: Optional[str] = Field(None, max_length=200, description="ë°œì·Œë¬¸")
    
    # ë¶„ë¥˜ ì •ë³´
    primary_category: Optional[str] = None
    secondary_categories: List[str] = Field(default_factory=list)
    tags: List[str] = Field(default_factory=list)
    
    # ì™¸ë¶€ ë§í¬
    source_url: Optional[HttpUrl] = None
    related_urls: List[HttpUrl] = Field(default_factory=list)
    
    # ì‘ì„±ì ì •ë³´
    author: Optional[str] = None
    organization: Optional[str] = None
    
    # ì–¸ì–´ ë° ì§€ì—­
    language: str = Field(default="ko", description="ì–¸ì–´ ì½”ë“œ")
    target_audience: List[str] = Field(default_factory=list, description="ëŒ€ìƒ ë…ì")
    
    # ìƒíƒœ ê´€ë¦¬
    status: ContentStatus = Field(default=ContentStatus.DRAFT)
    is_featured: bool = Field(default=False, description="ì¶”ì²œ ì½˜í…ì¸  ì—¬ë¶€")
    is_public: bool = Field(default=True, description="ê³µê°œ ì—¬ë¶€")
    
    # ê²€ìƒ‰ ìµœì í™”
    keywords: List[str] = Field(default_factory=list)
    search_boost: float = Field(default=1.0, ge=0.1, le=10.0, description="ê²€ìƒ‰ ê°€ì¤‘ì¹˜")

class Content(ContentBase):
    """MongoDBìš© ì½˜í…ì¸  ëª¨ë¸"""
    id: Optional[str] = Field(alias="_id")
    
    # ìë™ ìƒì„± ë©”íƒ€ë°ì´í„°
    word_count: int = Field(default=0, description="ë‹¨ì–´ ìˆ˜")
    reading_time_minutes: int = Field(default=0, description="ì˜ˆìƒ ì½ê¸° ì‹œê°„")
    content_hash: Optional[str] = None
    
    # AI ë¶„ì„ ê²°ê³¼
    ai_categories: List[Dict[str, Any]] = Field(default_factory=list)
    ai_tags: List[Dict[str, Any]] = Field(default_factory=list)
    ai_sentiment: Optional[Dict[str, Any]] = None
    ai_summary: Optional[str] = None
    
    # í’ˆì§ˆ ì§€í‘œ
    quality_score: Optional[float] = Field(None, ge=0, le=1)
    quality_metrics: Optional[Dict[str, Any]] = None
    
    # ìƒí˜¸ì‘ìš© ì§€í‘œ
    view_count: int = Field(default=0)
    like_count: int = Field(default=0)
    share_count: int = Field(default=0)
    download_count: int = Field(default=0)
    
    # ì¶”ì²œ ê´€ë ¨
    recommendation_score: Optional[float] = Field(None, ge=0, le=1)
    similar_content_ids: List[str] = Field(default_factory=list)
    
    # ì‹œê°„ ì •ë³´
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    published_at: Optional[datetime] = None
    last_viewed_at: Optional[datetime] = None
    
    # ë²„ì „ ê´€ë¦¬
    version: int = Field(default=1)
    revision_history: List[Dict[str, Any]] = Field(default_factory=list)
    
    class Config:
        allow_population_by_field_name = True
        json_encoders = {
            ObjectId: str,
            datetime: lambda v: v.isoformat()
        }
        schema_extra = {
            "example": {
                "title": "ê³µê³µë°ì´í„° API í™œìš© ê°€ì´ë“œ",
                "content": "ê³µê³µë°ì´í„° APIë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìƒì„¸ ê°€ì´ë“œ...",
                "content_type": "guide",
                "summary": "ê³µê³µë°ì´í„° API í™œìš©ì„ ìœ„í•œ ë‹¨ê³„ë³„ ì•ˆë‚´ì„œ",
                "primary_category": "tutorial",
                "secondary_categories": ["api", "development"],
                "tags": ["API", "ê³µê³µë°ì´í„°", "ê°œë°œ", "íŠœí† ë¦¬ì–¼"],
                "language": "ko",
                "target_audience": ["ê°œë°œì", "ë°ì´í„° ë¶„ì„ê°€"],
                "keywords": ["API", "REST", "JSON", "ì¸ì¦"]
            }
        }

    @validator('content')
    def validate_content_length(cls, v):
        if len(v.split()) < 10:
            raise ValueError('ì½˜í…ì¸ ëŠ” ìµœì†Œ 10ê°œ ë‹¨ì–´ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤')
        return v
    
    @validator('tags')
    def validate_tags(cls, v):
        if len(v) > 20:
            raise ValueError('íƒœê·¸ëŠ” ìµœëŒ€ 20ê°œê¹Œì§€ í—ˆìš©ë©ë‹ˆë‹¤')
        return v

class ContentMetrics(BaseModel):
    """ì½˜í…ì¸  ë©”íŠ¸ë¦­ ëª¨ë¸"""
    content_id: str
    
    # ì°¸ì—¬ ì§€í‘œ
    views: int = 0
    unique_views: int = 0
    likes: int = 0
    shares: int = 0
    downloads: int = 0
    bookmarks: int = 0
    
    # ì½ê¸° í–‰ë™
    avg_read_time: float = 0
    bounce_rate: float = 0
    completion_rate: float = 0
    
    # ê²€ìƒ‰ ì„±ê³¼
    search_impressions: int = 0
    search_clicks: int = 0
    search_ctr: float = 0
    
    # ì¶”ì²œ ì„±ê³¼
    recommendation_impressions: int = 0
    recommendation_clicks: int = 0
    recommendation_ctr: float = 0
    
    # ì‹œê°„ë³„ ì§‘ê³„
    daily_metrics: Dict[str, Dict[str, int]] = Field(default_factory=dict)
    weekly_metrics: Dict[str, Dict[str, int]] = Field(default_factory=dict)
    monthly_metrics: Dict[str, Dict[str, int]] = Field(default_factory=dict)
    
    last_updated: datetime = Field(default_factory=datetime.utcnow)

class ContentCategory(BaseModel):
    """ì½˜í…ì¸  ì¹´í…Œê³ ë¦¬ ëª¨ë¸"""
    id: Optional[str] = Field(alias="_id")
    name: str = Field(..., min_length=2, max_length=50)
    slug: str = Field(..., regex=r'^[a-z0-9-]+$')
    description: Optional[str] = None
    
    # ê³„ì¸µ êµ¬ì¡°
    parent_id: Optional[str] = None
    level: int = Field(default=0, ge=0, le=5)
    sort_order: int = Field(default=0)
    
    # ë©”íƒ€ë°ì´í„°
    color: Optional[str] = Field(None, regex=r'^#[0-9a-fA-F]{6}$')
    icon: Optional[str] = None
    
    # í†µê³„
    content_count: int = Field(default=0)
    is_active: bool = Field(default=True)
    
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

class ContentTag(BaseModel):
    """ì½˜í…ì¸  íƒœê·¸ ëª¨ë¸"""
    id: Optional[str] = Field(alias="_id")
    name: str = Field(..., min_length=1, max_length=30)
    slug: str = Field(..., regex=r'^[a-z0-9-ê°€-í£]+$')
    
    # íƒœê·¸ íƒ€ì…
    tag_type: str = Field(default="general")  # general, technical, topic, skill
    
    # ë©”íƒ€ë°ì´í„°
    description: Optional[str] = None
    color: Optional[str] = None
    
    # í†µê³„ ë° ê°€ì¤‘ì¹˜
    usage_count: int = Field(default=0)
    relevance_score: float = Field(default=1.0, ge=0, le=1)
    
    # ê´€ë ¨ íƒœê·¸
    related_tags: List[str] = Field(default_factory=list)
    synonyms: List[str] = Field(default_factory=list)
    
    is_active: bool = Field(default=True)
    created_at: datetime = Field(default_factory=datetime.utcnow)
```

### ê²€ìƒ‰ ë° í•„í„°ë§ ëª¨ë¸
```python
# app/domains/contents/schemas/request.py

from typing import Optional, List, Union
from pydantic import BaseModel, Field
from datetime import datetime

class ContentFilters(BaseModel):
    """ì½˜í…ì¸  í•„í„°ë§ íŒŒë¼ë¯¸í„°"""
    # ê¸°ë³¸ í•„í„°
    content_type: Optional[str] = None
    status: Optional[str] = Field(default="published")
    language: Optional[str] = None
    
    # ì¹´í…Œê³ ë¦¬ ë° íƒœê·¸ í•„í„°
    primary_category: Optional[str] = None
    categories: Optional[List[str]] = None
    tags: Optional[List[str]] = None
    
    # ì‘ì„±ì í•„í„°
    author: Optional[str] = None
    organization: Optional[str] = None
    
    # í’ˆì§ˆ í•„í„°
    min_quality_score: Optional[float] = Field(None, ge=0, le=1)
    min_word_count: Optional[int] = Field(None, ge=0)
    max_word_count: Optional[int] = Field(None, ge=0)
    
    # ë‚ ì§œ í•„í„°
    created_after: Optional[datetime] = None
    created_before: Optional[datetime] = None
    published_after: Optional[datetime] = None
    published_before: Optional[datetime] = None
    
    # ì¸ê¸°ë„ í•„í„°
    min_view_count: Optional[int] = Field(None, ge=0)
    min_like_count: Optional[int] = Field(None, ge=0)
    is_featured: Optional[bool] = None
    
    # ê²€ìƒ‰ ì¿¼ë¦¬
    search_query: Optional[str] = Field(None, min_length=2)
    search_fields: List[str] = Field(
        default=["title", "content", "summary", "tags"],
        description="ê²€ìƒ‰ ëŒ€ìƒ í•„ë“œ"
    )
    
    # ì •ë ¬
    sort_by: str = Field(
        default="created_at",
        regex="^(created_at|updated_at|published_at|view_count|like_count|quality_score|relevance)$"
    )
    sort_order: str = Field(default="desc", regex="^(asc|desc)$")
    
    # í˜ì´ì§•
    page: int = Field(default=1, ge=1)
    size: int = Field(default=20, ge=1, le=100)
    
    class Config:
        schema_extra = {
            "example": {
                "content_type": "guide",
                "categories": ["api", "tutorial"],
                "tags": ["ê°œë°œ", "API"],
                "min_quality_score": 0.7,
                "search_query": "ê³µê³µë°ì´í„° í™œìš©",
                "sort_by": "relevance",
                "page": 1,
                "size": 20
            }
        }

class ContentSearchRequest(BaseModel):
    """ê³ ê¸‰ ê²€ìƒ‰ ìš”ì²­"""
    query: str = Field(..., min_length=1, description="ê²€ìƒ‰ ì¿¼ë¦¬")
    
    # ê²€ìƒ‰ íƒ€ì…
    search_type: str = Field(
        default="full_text",
        regex="^(full_text|semantic|hybrid)$",
        description="ê²€ìƒ‰ ë°©ì‹"
    )
    
    # ê²€ìƒ‰ ì˜µì…˜
    fuzzy: bool = Field(default=True, description="ìœ ì‚¬ ê²€ìƒ‰ í—ˆìš©")
    highlight: bool = Field(default=True, description="ê²€ìƒ‰ì–´ í•˜ì´ë¼ì´íŒ…")
    include_summary: bool = Field(default=True, description="AI ìš”ì•½ í¬í•¨")
    
    # í•„í„°ë§
    filters: Optional[ContentFilters] = None
    
    # ê²°ê³¼ ì˜µì…˜
    include_similar: bool = Field(default=False, description="ìœ ì‚¬ ì½˜í…ì¸  í¬í•¨")
    max_results: int = Field(default=50, ge=1, le=100)
    
    class Config:
        schema_extra = {
            "example": {
                "query": "API ì¸ì¦ ë°©ë²•",
                "search_type": "hybrid",
                "fuzzy": True,
                "highlight": True,
                "filters": {
                    "content_type": "guide",
                    "min_quality_score": 0.8
                },
                "max_results": 20
            }
        }

class ContentRecommendationRequest(BaseModel):
    """ì½˜í…ì¸  ì¶”ì²œ ìš”ì²­"""
    user_id: Optional[str] = None
    content_id: Optional[str] = Field(None, description="ê¸°ì¤€ ì½˜í…ì¸  ID")
    
    # ì¶”ì²œ ê¸°ì¤€
    recommendation_type: str = Field(
        default="personalized",
        regex="^(personalized|similar|popular|trending)$"
    )
    
    # ì‚¬ìš©ì ì„ í˜¸ë„ (ì˜µì…˜)
    preferred_categories: Optional[List[str]] = None
    preferred_tags: Optional[List[str]] = None
    preferred_content_types: Optional[List[str]] = None
    
    # ì œì™¸ ì¡°ê±´
    exclude_content_ids: Optional[List[str]] = None
    exclude_categories: Optional[List[str]] = None
    
    # ê²°ê³¼ ì œí•œ
    limit: int = Field(default=10, ge=1, le=50)
    min_score: float = Field(default=0.5, ge=0, le=1)
    
    class Config:
        schema_extra = {
            "example": {
                "user_id": "user123",
                "recommendation_type": "personalized",
                "preferred_categories": ["tutorial", "guide"],
                "preferred_tags": ["API", "ê°œë°œ"],
                "limit": 10
            }
        }
```

## ğŸ”§ API ì—”ë“œí¬ì¸íŠ¸

### CRUD ì—”ë“œí¬ì¸íŠ¸
```python
# app/domains/contents/routers/content_router.py

from fastapi import APIRouter, Depends, HTTPException, Query, UploadFile, File
from fastapi_pagination import Page, Params
from typing import List, Optional, Dict, Any

from ..services.content_service import ContentService
from ..services.search_service import SearchService
from ..services.recommendation_service import RecommendationService
from ..schemas.request import ContentFilters, ContentSearchRequest, ContentRecommendationRequest
from ..schemas.response import ContentResponse, SearchResultResponse, RecommendationResponse
from ...auth.dependencies import get_current_user

router = APIRouter(prefix="/api/v1/contents", tags=["contents"])

@router.get("/", response_model=Page[ContentResponse])
async def get_contents(
    filters: ContentFilters = Depends(),
    service: ContentService = Depends(),
    params: Params = Depends()
) -> Page[ContentResponse]:
    """
    ì½˜í…ì¸  ëª©ë¡ ì¡°íšŒ (ê³ ê¸‰ í•„í„°ë§ ì§€ì›)
    
    ### í•„í„°ë§ ì˜µì…˜:
    - **content_type**: ì½˜í…ì¸  íƒ€ì…ë³„ í•„í„°ë§
    - **categories**: ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§
    - **tags**: íƒœê·¸ë³„ í•„í„°ë§
    - **quality_score**: í’ˆì§ˆ ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§
    - **search_query**: ì „ë¬¸ ê²€ìƒ‰
    
    ### ì •ë ¬ ì˜µì…˜:
    - **created_at**: ìƒì„±ì¼ìˆœ
    - **published_at**: ê²Œì‹œì¼ìˆœ
    - **view_count**: ì¡°íšŒìˆ˜ìˆœ
    - **quality_score**: í’ˆì§ˆ ì ìˆ˜ìˆœ
    - **relevance**: ê´€ë ¨ë„ìˆœ (ê²€ìƒ‰ ì‹œ)
    """
    return await service.get_contents_with_filters(filters, params)

@router.get("/{content_id}", response_model=ContentResponse)
async def get_content_detail(
    content_id: str,
    include_similar: bool = Query(default=False, description="ìœ ì‚¬ ì½˜í…ì¸  í¬í•¨"),
    track_view: bool = Query(default=True, description="ì¡°íšŒìˆ˜ ê¸°ë¡"),
    service: ContentService = Depends()
) -> ContentResponse:
    """
    ì½˜í…ì¸  ìƒì„¸ ì¡°íšŒ
    
    - ì½˜í…ì¸  ë³¸ë¬¸ ë° ë©”íƒ€ë°ì´í„°
    - AI ë¶„ì„ ê²°ê³¼ (ì¹´í…Œê³ ë¦¬, íƒœê·¸, í’ˆì§ˆ ì ìˆ˜)
    - ìœ ì‚¬ ì½˜í…ì¸  ì¶”ì²œ (ì˜µì…˜)
    - ì¡°íšŒìˆ˜ ìë™ ì¦ê°€ (ì˜µì…˜)
    """
    content = await service.get_content_by_id(content_id, track_view)
    if not content:
        raise HTTPException(status_code=404, detail="Content not found")
    
    if include_similar:
        similar_contents = await service.get_similar_contents(content_id, limit=5)
        content.similar_contents = similar_contents
    
    return content

@router.post("/", response_model=ContentResponse, status_code=201)
async def create_content(
    content_data: ContentCreate,
    auto_classify: bool = Query(default=True, description="ìë™ ë¶„ë¥˜ ì‹¤í–‰"),
    service: ContentService = Depends(),
    current_user: dict = Depends(get_current_user)
) -> ContentResponse:
    """
    ìƒˆ ì½˜í…ì¸  ìƒì„±
    
    - ë°ì´í„° ê²€ì¦ ë° ì¤‘ë³µ ì²´í¬
    - ìë™ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    - AI ê¸°ë°˜ ìë™ ë¶„ë¥˜ (ì˜µì…˜)
    - í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
    """
    # ì¤‘ë³µ ì²´í¬
    existing = await service.check_duplicate_content(content_data.title, content_data.content)
    if existing:
        raise HTTPException(status_code=409, detail="Similar content already exists")
    
    content = await service.create_content(content_data, current_user["id"])
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… íì‰
    if auto_classify:
        from ..tasks.classification_tasks import classify_content
        classify_content.delay(content.id)
    
    return content

@router.put("/{content_id}", response_model=ContentResponse)
async def update_content(
    content_id: str,
    content_data: ContentUpdate,
    reclassify: bool = Query(default=False, description="ì¬ë¶„ë¥˜ ì‹¤í–‰"),
    service: ContentService = Depends(),
    current_user: dict = Depends(get_current_user)
) -> ContentResponse:
    """ì½˜í…ì¸  ìˆ˜ì •"""
    content = await service.update_content(content_id, content_data, current_user["id"])
    if not content:
        raise HTTPException(status_code=404, detail="Content not found")
    
    # ì¬ë¶„ë¥˜ ìš”ì²­ ì‹œ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹¤í–‰
    if reclassify:
        from ..tasks.classification_tasks import reclassify_content
        reclassify_content.delay(content_id)
    
    return content

@router.delete("/{content_id}", status_code=204)
async def delete_content(
    content_id: str,
    service: ContentService = Depends(),
    current_user: dict = Depends(get_current_user)
):
    """ì½˜í…ì¸  ì‚­ì œ (ì†Œí”„íŠ¸ ì‚­ì œ)"""
    success = await service.soft_delete_content(content_id)
    if not success:
        raise HTTPException(status_code=404, detail="Content not found")

@router.post("/bulk-upload", response_model=List[ContentResponse])
async def bulk_upload_contents(
    files: List[UploadFile] = File(...),
    auto_process: bool = Query(default=True, description="ìë™ ì²˜ë¦¬ ì‹¤í–‰"),
    service: ContentService = Depends(),
    current_user: dict = Depends(get_current_user)
) -> List[ContentResponse]:
    """
    ëŒ€ëŸ‰ ì½˜í…ì¸  ì—…ë¡œë“œ
    
    - ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ ì§€ì›
    - ìë™ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - ë°°ì¹˜ ì²˜ë¦¬ ë° ë¶„ë¥˜
    """
    if len(files) > 50:
        raise HTTPException(status_code=400, detail="Maximum 50 files allowed")
    
    results = []
    for file in files:
        try:
            content = await service.create_content_from_file(file, current_user["id"], auto_process)
            results.append(content)
        except Exception as e:
            # ê°œë³„ íŒŒì¼ ì‹¤íŒ¨ëŠ” ë¡œê·¸ë§Œ ê¸°ë¡í•˜ê³  ê³„ì† ì§„í–‰
            await service.log_upload_error(file.filename, str(e))
    
    return results
```

### ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸
```python
@router.post("/search", response_model=SearchResultResponse)
async def search_contents(
    search_request: ContentSearchRequest,
    search_service: SearchService = Depends()
) -> SearchResultResponse:
    """
    ê³ ê¸‰ ì½˜í…ì¸  ê²€ìƒ‰
    
    ### ê²€ìƒ‰ ë°©ì‹:
    - **full_text**: ì „ë¬¸ ê²€ìƒ‰ (í‚¤ì›Œë“œ ë§¤ì¹­)
    - **semantic**: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„)
    - **hybrid**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì „ë¬¸ + ì˜ë¯¸)
    
    ### ê¸°ëŠ¥:
    - ìœ ì‚¬ ê²€ìƒ‰ (fuzzy matching)
    - ê²€ìƒ‰ì–´ í•˜ì´ë¼ì´íŒ…
    - AI ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
    - í•„í„°ë§ ë° ì •ë ¬
    """
    search_results = await search_service.search_contents(search_request)
    
    # ê²€ìƒ‰ ë¡œê·¸ ê¸°ë¡
    await search_service.log_search_event(
        query=search_request.query,
        search_type=search_request.search_type,
        results_count=len(search_results.results),
        user_id=search_request.user_id if hasattr(search_request, 'user_id') else None
    )
    
    return search_results

@router.get("/search/suggestions")
async def get_search_suggestions(
    query: str = Query(..., min_length=1, description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    limit: int = Query(default=10, ge=1, le=20),
    search_service: SearchService = Depends()
) -> List[str]:
    """
    ê²€ìƒ‰ì–´ ìë™ì™„ì„± ì œì•ˆ
    
    - ì¸ê¸° ê²€ìƒ‰ì–´ ê¸°ë°˜ ì œì•ˆ
    - íƒœê·¸ ë° ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
    - ìœ ì‚¬ ê²€ìƒ‰ì–´ ì œì•ˆ
    """
    suggestions = await search_service.get_search_suggestions(query, limit)
    return suggestions

@router.get("/search/trending")
async def get_trending_searches(
    period: str = Query(default="7d", regex="^(1d|7d|30d)$"),
    limit: int = Query(default=20, ge=1, le=50),
    search_service: SearchService = Depends()
) -> List[Dict[str, Any]]:
    """
    ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ
    
    - ê¸°ê°„ë³„ ì¸ê¸° ê²€ìƒ‰ì–´
    - ê²€ìƒ‰ ë¹ˆë„ ë° íŠ¸ë Œë“œ
    - ì¹´í…Œê³ ë¦¬ë³„ ì¸ê¸° ê²€ìƒ‰ì–´
    """
    trending = await search_service.get_trending_searches(period, limit)
    return trending
```

### ì¶”ì²œ ì—”ë“œí¬ì¸íŠ¸
```python
@router.post("/recommendations", response_model=List[RecommendationResponse])
async def get_content_recommendations(
    recommendation_request: ContentRecommendationRequest,
    recommendation_service: RecommendationService = Depends()
) -> List[RecommendationResponse]:
    """
    ê°œì¸í™” ì½˜í…ì¸  ì¶”ì²œ
    
    ### ì¶”ì²œ ë°©ì‹:
    - **personalized**: ì‚¬ìš©ì í–‰ë™ ê¸°ë°˜ ê°œì¸í™”
    - **similar**: ì½˜í…ì¸  ìœ ì‚¬ë„ ê¸°ë°˜
    - **popular**: ì¸ê¸° ì½˜í…ì¸  ê¸°ë°˜
    - **trending**: íŠ¸ë Œë”© ì½˜í…ì¸  ê¸°ë°˜
    
    ### ê¸°ëŠ¥:
    - í˜‘ì—… í•„í„°ë§
    - ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§
    - í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ
    - ì‹¤ì‹œê°„ ê°œì¸í™”
    """
    recommendations = await recommendation_service.get_recommendations(recommendation_request)
    
    # ì¶”ì²œ ë¡œê·¸ ê¸°ë¡
    await recommendation_service.log_recommendation_event(
        user_id=recommendation_request.user_id,
        recommendation_type=recommendation_request.recommendation_type,
        content_ids=[rec.content.id for rec in recommendations]
    )
    
    return recommendations

@router.get("/{content_id}/similar", response_model=List[ContentResponse])
async def get_similar_contents(
    content_id: str,
    limit: int = Query(default=10, ge=1, le=20),
    similarity_threshold: float = Query(default=0.6, ge=0, le=1),
    service: ContentService = Depends()
) -> List[ContentResponse]:
    """
    ìœ ì‚¬ ì½˜í…ì¸  ì¡°íšŒ
    
    - ì½˜í…ì¸  ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜
    - ì¹´í…Œê³ ë¦¬ ë° íƒœê·¸ ìœ ì‚¬ë„
    - ML ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
    """
    similar_contents = await service.get_similar_contents(
        content_id, limit, similarity_threshold
    )
    return similar_contents

@router.get("/recommendations/popular")
async def get_popular_contents(
    period: str = Query(default="7d", regex="^(1d|7d|30d)$"),
    category: Optional[str] = Query(None),
    content_type: Optional[str] = Query(None),
    limit: int = Query(default=20, ge=1, le=50),
    service: ContentService = Depends()
) -> List[ContentResponse]:
    """
    ì¸ê¸° ì½˜í…ì¸  ì¡°íšŒ
    
    - ê¸°ê°„ë³„ ì¸ê¸° ì½˜í…ì¸ 
    - ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§
    - ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ê³µìœ ìˆ˜ ê¸°ë°˜ ë­í‚¹
    """
    popular_contents = await service.get_popular_contents(
        period, category, content_type, limit
    )
    return popular_contents
```

## ğŸ¤– AI ë¶„ë¥˜ ì‹œìŠ¤í…œ

### BERT ê¸°ë°˜ ë¶„ë¥˜ê¸°
```python
# app/domains/contents/ml/bert_classifier.py

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import List, Dict, Any, Tuple
import numpy as np
from sklearn.preprocessing import LabelEncoder
import asyncio
import logging

class BERTContentClassifier:
    """BERT ê¸°ë°˜ ì½˜í…ì¸  ìë™ ë¶„ë¥˜ê¸°"""
    
    def __init__(self, model_name: str = "klue/bert-base"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.label_encoder = LabelEncoder()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.is_loaded = False
        
        # ì‚¬ì „ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬
        self.categories = [
            "APIê°œë°œ", "ë°ì´í„°ë¶„ì„", "ë¨¸ì‹ ëŸ¬ë‹", "ì›¹ê°œë°œ", "ëª¨ë°”ì¼ì•±",
            "ë¹…ë°ì´í„°", "í´ë¼ìš°ë“œ", "ë³´ì•ˆ", "ë¸”ë¡ì²´ì¸", "IoT",
            "ì •ì±…ê°€ì´ë“œ", "ì‚¬ì—…ì§€ì›", "êµìœ¡ìë£Œ", "ê¸°ìˆ ë™í–¥", "ì‚¬ë¡€ì—°êµ¬"
        ]
        
        self.confidence_threshold = 0.7
        self.max_length = 512
    
    async def load_model(self):
        """ëª¨ë¸ ë¡œë”© (ë¹„ë™ê¸°)"""
        if self.is_loaded:
            return
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                num_labels=len(self.categories)
            )
            self.model.to(self.device)
            self.model.eval()
            
            # ë ˆì´ë¸” ì¸ì½”ë” ì„¤ì •
            self.label_encoder.fit(self.categories)
            
            self.is_loaded = True
            logging.info(f"BERT classifier loaded on {self.device}")
            
        except Exception as e:
            logging.error(f"Failed to load BERT model: {e}")
            raise
    
    async def classify_content(
        self, title: str, content: str, summary: str = None
    ) -> List[Dict[str, Any]]:
        """ì½˜í…ì¸  ë¶„ë¥˜ ì‹¤í–‰"""
        if not self.is_loaded:
            await self.load_model()
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = self._prepare_text(title, content, summary)
        
        # í† í°í™”
        inputs = self.tokenizer(
            text,
            max_length=self.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # ê²°ê³¼ ì²˜ë¦¬
        predictions_np = predictions.cpu().numpy()[0]
        results = []
        
        # ìƒìœ„ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ
        top_indices = np.argsort(predictions_np)[::-1]
        
        for idx in top_indices:
            confidence = float(predictions_np[idx])
            if confidence >= self.confidence_threshold:
                category = self.categories[idx]
                results.append({
                    "category": category,
                    "confidence": confidence,
                    "rank": len(results) + 1
                })
            
            # ìµœëŒ€ 5ê°œ ì¹´í…Œê³ ë¦¬ê¹Œì§€
            if len(results) >= 5:
                break
        
        return results
    
    async def classify_batch(
        self, contents: List[Dict[str, str]]
    ) -> List[List[Dict[str, Any]]]:
        """ë°°ì¹˜ ë¶„ë¥˜"""
        if not self.is_loaded:
            await self.load_model()
        
        # ë°°ì¹˜ í…ìŠ¤íŠ¸ ì¤€ë¹„
        texts = [
            self._prepare_text(
                content.get("title", ""),
                content.get("content", ""),
                content.get("summary")
            )
            for content in contents
        ]
        
        # ë°°ì¹˜ í† í°í™”
        inputs = self.tokenizer(
            texts,
            max_length=self.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ë°°ì¹˜ ì¶”ë¡ 
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # ê²°ê³¼ ì²˜ë¦¬
        predictions_np = predictions.cpu().numpy()
        results = []
        
        for i, pred in enumerate(predictions_np):
            content_results = []
            top_indices = np.argsort(pred)[::-1]
            
            for idx in top_indices:
                confidence = float(pred[idx])
                if confidence >= self.confidence_threshold:
                    category = self.categories[idx]
                    content_results.append({
                        "category": category,
                        "confidence": confidence,
                        "rank": len(content_results) + 1
                    })
                
                if len(content_results) >= 5:
                    break
            
            results.append(content_results)
        
        return results
    
    def _prepare_text(self, title: str, content: str, summary: str = None) -> str:
        """ë¶„ë¥˜ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ì œëª©ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ (2ë°°)
        text_parts = [title, title]
        
        # ìš”ì•½ì´ ìˆìœ¼ë©´ ì¶”ê°€ (1.5ë°° ê°€ì¤‘ì¹˜)
        if summary:
            text_parts.extend([summary, summary[:len(summary)//2]])
        
        # ë³¸ë¬¸ ì¶”ê°€ (ê¸¸ì´ ì œí•œ)
        content_preview = content[:1000] if len(content) > 1000 else content
        text_parts.append(content_preview)
        
        return " ".join(filter(None, text_parts))
    
    async def fine_tune(
        self, training_data: List[Dict[str, Any]], epochs: int = 3
    ):
        """íŒŒì¸íŠœë‹ (ì¶”ê°€ í•™ìŠµ)"""
        # TODO: íŒŒì¸íŠœë‹ êµ¬í˜„
        pass
    
    async def evaluate_model(self, test_data: List[Dict[str, Any]]) -> Dict[str, float]:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        # TODO: í‰ê°€ ë©”íŠ¸ë¦­ êµ¬í˜„
        pass

class TagExtractor:
    """í‚¤ì›Œë“œ/íƒœê·¸ ìë™ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.min_frequency = 2
        self.max_tags = 20
        
        # ë¶ˆìš©ì–´ ì •ì˜
        self.stopwords = {
            "ê²ƒ", "ìˆ˜", "ë“±", "ìœ„í•´", "í†µí•´", "ëŒ€í•œ", "ìœ„í•œ", "ê°™ì€", "ìˆëŠ”",
            "ìˆë‹¤", "í•˜ëŠ”", "í•œë‹¤", "ëœë‹¤", "ë˜ëŠ”", "ì´ë‹¤", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜",
            "ë˜í•œ", "ë•Œë¬¸ì—", "ë”°ë¼ì„œ", "ê·¸ëŸ°ë°", "í•˜ì§€ë§Œ", "ê·¸ë˜ì„œ"
        }
    
    async def extract_tags(
        self, title: str, content: str, existing_tags: List[str] = None
    ) -> List[Dict[str, Any]]:
        """íƒœê·¸ ìë™ ì¶”ì¶œ"""
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = f"{title} {content}"
        processed_text = self._preprocess_text(text)
        
        # ëª…ì‚¬ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ í•„ìš”)
        nouns = await self._extract_nouns(processed_text)
        
        # ë¹ˆë„ ê³„ì‚°
        tag_frequencies = self._calculate_frequencies(nouns)
        
        # ê¸°ì¡´ íƒœê·¸ì™€ ì¤‘ë³µ ì œê±°
        if existing_tags:
            tag_frequencies = {
                tag: freq for tag, freq in tag_frequencies.items()
                if tag not in existing_tags
            }
        
        # ìƒìœ„ íƒœê·¸ ì„ ë³„
        top_tags = sorted(
            tag_frequencies.items(),
            key=lambda x: x[1],
            reverse=True
        )[:self.max_tags]
        
        # ê²°ê³¼ ìƒì„±
        results = []
        for tag, frequency in top_tags:
            if frequency >= self.min_frequency:
                confidence = min(frequency / len(nouns), 1.0)
                results.append({
                    "tag": tag,
                    "frequency": frequency,
                    "confidence": confidence,
                    "type": "extracted"
                })
        
        return results
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        import re
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    async def _extract_nouns(self, text: str) -> List[str]:
        """ëª…ì‚¬ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„)"""
        # TODO: KoNLPy ë˜ëŠ” ë‹¤ë¥¸ í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©
        # í˜„ì¬ëŠ” ê°„ë‹¨í•œ í† í°í™”ë¡œ ëŒ€ì²´
        tokens = text.split()
        
        # ê¸¸ì´ í•„í„°ë§
        filtered_tokens = [
            token for token in tokens
            if 2 <= len(token) <= 10 and token not in self.stopwords
        ]
        
        return filtered_tokens
    
    def _calculate_frequencies(self, words: List[str]) -> Dict[str, int]:
        """ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°"""
        frequencies = {}
        for word in words:
            frequencies[word] = frequencies.get(word, 0) + 1
        
        return frequencies
```

### í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
```python
# app/domains/contents/ml/quality_scorer.py

from typing import Dict, Any, List
import re
from datetime import datetime
import asyncio
import math

class ContentQualityScorer:
    """ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.weights = {
            "readability": 0.25,     # ê°€ë…ì„±
            "completeness": 0.20,    # ì™„ì„±ë„
            "accuracy": 0.20,        # ì •í™•ì„±
            "freshness": 0.15,       # ìµœì‹ ì„±
            "engagement": 0.20       # ì°¸ì—¬ë„
        }
        
        self.min_word_count = 100
        self.ideal_word_count = 1000
        self.max_sentence_length = 50
    
    async def calculate_quality_score(
        self, content: Dict[str, Any], metrics: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        
        scores = {
            "readability": await self._calculate_readability(content),
            "completeness": await self._calculate_completeness(content),
            "accuracy": await self._calculate_accuracy(content),
            "freshness": await self._calculate_freshness(content),
            "engagement": await self._calculate_engagement(content, metrics or {})
        }
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        overall_score = sum(
            scores[metric] * self.weights[metric]
            for metric in scores
        )
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        quality_grade = self._determine_quality_grade(overall_score)
        
        # ê°œì„  ì œì•ˆ ìƒì„±
        recommendations = await self._generate_recommendations(scores, content)
        
        return {
            "overall_score": round(overall_score, 3),
            "quality_grade": quality_grade,
            "scores": scores,
            "recommendations": recommendations,
            "calculated_at": datetime.utcnow(),
            "version": "2.0"
        }
    
    async def _calculate_readability(self, content: Dict[str, Any]) -> float:
        """ê°€ë…ì„± ì ìˆ˜ ê³„ì‚°"""
        text = content.get("content", "")
        
        # ê¸°ë³¸ í†µê³„
        words = text.split()
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not words or not sentences:
            return 0.0
        
        # í‰ê·  ë¬¸ì¥ ê¸¸ì´
        avg_sentence_length = len(words) / len(sentences)
        sentence_score = max(0, 1 - (avg_sentence_length - 20) / 30)
        
        # ë‹¨ì–´ ë³µì¡ë„ (í‰ê·  ë‹¨ì–´ ê¸¸ì´)
        avg_word_length = sum(len(word) for word in words) / len(words)
        word_score = max(0, 1 - (avg_word_length - 4) / 6)
        
        # ë¬¸ë‹¨ êµ¬ì¡°
        paragraphs = text.split('\n\n')
        paragraph_score = min(1.0, len(paragraphs) / 10)
        
        # ëª©ë¡ ë° êµ¬ì¡°í™” ìš”ì†Œ
        structure_elements = len(re.findall(r'[*\-â€¢]|\d+\.', text))
        structure_score = min(1.0, structure_elements / 20)
        
        readability = (
            sentence_score * 0.3 +
            word_score * 0.3 +
            paragraph_score * 0.2 +
            structure_score * 0.2
        )
        
        return max(0.0, min(1.0, readability))
    
    async def _calculate_completeness(self, content: Dict[str, Any]) -> float:
        """ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # í•„ìˆ˜ í•„ë“œ ì™„ì„±ë„
        required_fields = ["title", "content", "summary"]
        completed_fields = sum(1 for field in required_fields if content.get(field))
        field_score = completed_fields / len(required_fields)
        
        # ì½˜í…ì¸  ê¸¸ì´ ì ì ˆì„±
        word_count = len(content.get("content", "").split())
        if word_count < self.min_word_count:
            length_score = word_count / self.min_word_count
        elif word_count > self.ideal_word_count:
            length_score = max(0.7, 1 - (word_count - self.ideal_word_count) / 2000)
        else:
            length_score = 1.0
        
        # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
        meta_fields = ["tags", "categories", "author", "source_url"]
        meta_score = sum(1 for field in meta_fields if content.get(field)) / len(meta_fields)
        
        # êµ¬ì¡°í™” ìš”ì†Œ (ì œëª©, ëª©ë¡, ë§í¬ ë“±)
        text = content.get("content", "")
        structure_elements = (
            len(re.findall(r'^#+\s', text, re.MULTILINE)) +  # í—¤ë”©
            len(re.findall(r'[*\-â€¢]', text)) +               # ëª©ë¡
            len(re.findall(r'https?://', text))              # ë§í¬
        )
        structure_score = min(1.0, structure_elements / 15)
        
        completeness = (
            field_score * 0.4 +
            length_score * 0.3 +
            meta_score * 0.2 +
            structure_score * 0.1
        )
        
        return max(0.0, min(1.0, completeness))
    
    async def _calculate_accuracy(self, content: Dict[str, Any]) -> float:
        """ì •í™•ì„± ì ìˆ˜ ê³„ì‚°"""
        text = content.get("content", "")
        
        # ë§ì¶¤ë²• ë° ë¬¸ë²• ì²´í¬ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        grammar_score = await self._check_grammar_heuristics(text)
        
        # ì‚¬ì‹¤ í™•ì¸ ê°€ëŠ¥ì„± (ì¶œì²˜, ë§í¬ ë“±)
        sources = len(re.findall(r'https?://', text))
        citations = len(re.findall(r'\[.*?\]|\(.*?\)', text))
        source_score = min(1.0, (sources + citations) / 10)
        
        # ì¼ê´€ì„± ì²´í¬
        consistency_score = await self._check_consistency(content)
        
        accuracy = (
            grammar_score * 0.4 +
            source_score * 0.3 +
            consistency_score * 0.3
        )
        
        return max(0.0, min(1.0, accuracy))
    
    async def _calculate_freshness(self, content: Dict[str, Any]) -> float:
        """ìµœì‹ ì„± ì ìˆ˜ ê³„ì‚°"""
        now = datetime.utcnow()
        
        # ìƒì„±ì¼ ê¸°ì¤€ ìµœì‹ ì„±
        created_at = content.get("created_at")
        if isinstance(created_at, str):
            created_at = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
        elif not isinstance(created_at, datetime):
            return 0.5  # ê¸°ë³¸ê°’
        
        days_old = (now - created_at).days
        
        # ì½˜í…ì¸  íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        content_type = content.get("content_type", "")
        if content_type in ["news", "policy"]:
            # ë‰´ìŠ¤ë‚˜ ì •ì±…ì€ ë¹ ë¥´ê²Œ êµ¬ì‹ì´ ë¨
            freshness = max(0, 1 - days_old / 30)
        elif content_type in ["guide", "tutorial"]:
            # ê°€ì´ë“œë‚˜ íŠœí† ë¦¬ì–¼ì€ ìƒëŒ€ì ìœ¼ë¡œ ì˜¤ë˜ ìœ íš¨
            freshness = max(0, 1 - days_old / 365)
        else:
            # ì¼ë°˜ ì½˜í…ì¸ 
            freshness = max(0, 1 - days_old / 180)
        
        return freshness
    
    async def _calculate_engagement(
        self, content: Dict[str, Any], metrics: Dict[str, Any]
    ) -> float:
        """ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°"""
        if not metrics:
            return 0.5  # ê¸°ë³¸ê°’
        
        views = metrics.get("view_count", 0)
        likes = metrics.get("like_count", 0)
        shares = metrics.get("share_count", 0)
        
        # ì •ê·œí™”ëœ ì°¸ì—¬ ì§€í‘œ
        view_score = min(1.0, views / 1000)
        like_rate = likes / max(views, 1)
        share_rate = shares / max(views, 1)
        
        engagement = (
            view_score * 0.5 +
            min(1.0, like_rate * 20) * 0.3 +
            min(1.0, share_rate * 50) * 0.2
        )
        
        return engagement
    
    def _determine_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if score >= 0.9:
            return "Excellent"
        elif score >= 0.8:
            return "Good"
        elif score >= 0.7:
            return "Fair"
        elif score >= 0.6:
            return "Poor"
        else:
            return "Very Poor"
    
    async def _generate_recommendations(
        self, scores: Dict[str, float], content: Dict[str, Any]
    ) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        recommendations = []
        
        if scores["readability"] < 0.7:
            recommendations.append("ë¬¸ì¥ì„ ë” ì§§ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ë³´ì„¸ìš”")
            recommendations.append("ëª©ë¡ì´ë‚˜ ì†Œì œëª©ì„ í™œìš©í•˜ì—¬ êµ¬ì¡°í™”í•´ë³´ì„¸ìš”")
        
        if scores["completeness"] < 0.7:
            word_count = len(content.get("content", "").split())
            if word_count < self.min_word_count:
                recommendations.append(f"ì½˜í…ì¸  ê¸¸ì´ë¥¼ ëŠ˜ë ¤ì£¼ì„¸ìš” (í˜„ì¬: {word_count}ë‹¨ì–´)")
            
            if not content.get("summary"):
                recommendations.append("ì½˜í…ì¸  ìš”ì•½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”")
            
            if not content.get("tags"):
                recommendations.append("ê´€ë ¨ íƒœê·¸ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”")
        
        if scores["accuracy"] < 0.7:
            recommendations.append("ì¶œì²˜ë‚˜ ì°¸ê³  ë§í¬ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”")
            recommendations.append("ë‚´ìš©ì˜ ì •í™•ì„±ì„ ì¬ê²€í† í•´ì£¼ì„¸ìš”")
        
        if scores["freshness"] < 0.5:
            recommendations.append("ìµœì‹  ì •ë³´ë¡œ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”")
        
        return recommendations
    
    async def _check_grammar_heuristics(self, text: str) -> float:
        """ê°„ë‹¨í•œ ë¬¸ë²• ì²´í¬ íœ´ë¦¬ìŠ¤í‹±"""
        # ê¸°ë³¸ì ì¸ ë¬¸ë²• ì˜¤ë¥˜ íŒ¨í„´ ì²´í¬
        errors = 0
        
        # ì—°ì†ëœ ê³µë°±
        errors += len(re.findall(r'\s{3,}', text))
        
        # ì—°ì†ëœ êµ¬ë‘ì 
        errors += len(re.findall(r'[.!?]{2,}', text))
        
        # ë¬¸ì¥ ë¶€í˜¸ ì• ê³µë°±
        errors += len(re.findall(r'\s+[.!?]', text))
        
        # ì˜¤ë¥˜ ë¹„ìœ¨ ê³„ì‚°
        words = len(text.split())
        error_rate = errors / max(words, 1)
        
        return max(0, 1 - error_rate * 10)
    
    async def _check_consistency(self, content: Dict[str, Any]) -> float:
        """ì¼ê´€ì„± ì²´í¬"""
        title = content.get("title", "")
        text = content.get("content", "")
        tags = content.get("tags", [])
        
        # ì œëª©ê³¼ ë‚´ìš©ì˜ ì—°ê´€ì„±
        title_words = set(title.lower().split())
        content_words = set(text.lower().split())
        overlap = len(title_words & content_words) / max(len(title_words), 1)
        
        # íƒœê·¸ì™€ ë‚´ìš©ì˜ ì—°ê´€ì„±
        tag_overlap = 0
        if tags:
            tag_words = set(' '.join(tags).lower().split())
            tag_overlap = len(tag_words & content_words) / max(len(tag_words), 1)
        
        consistency = (overlap * 0.7 + tag_overlap * 0.3)
        return min(1.0, consistency)
```

## ğŸ” ê²€ìƒ‰ ì—”ì§„

### Elasticsearch í†µí•©
```python
# app/domains/contents/services/search_service.py

from elasticsearch import AsyncElasticsearch
from typing import List, Dict, Any, Optional
import json
from datetime import datetime, timedelta

class ContentSearchService:
    """ì½˜í…ì¸  ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    def __init__(self, es_client: AsyncElasticsearch):
        self.es = es_client
        self.index_name = "contents"
        self.vector_index = "content_vectors"
    
    async def search_contents(
        self, search_request: ContentSearchRequest
    ) -> SearchResultResponse:
        """í†µí•© ê²€ìƒ‰ ì‹¤í–‰"""
        
        if search_request.search_type == "full_text":
            results = await self._full_text_search(search_request)
        elif search_request.search_type == "semantic":
            results = await self._semantic_search(search_request)
        elif search_request.search_type == "hybrid":
            results = await self._hybrid_search(search_request)
        else:
            raise ValueError(f"Unknown search type: {search_request.search_type}")
        
        # ê²°ê³¼ í›„ì²˜ë¦¬
        if search_request.highlight:
            results = await self._add_highlights(results, search_request.query)
        
        if search_request.include_summary:
            results = await self._add_ai_summaries(results, search_request.query)
        
        return SearchResultResponse(
            query=search_request.query,
            search_type=search_request.search_type,
            total_results=len(results),
            results=results,
            search_time=datetime.utcnow(),
            suggestions=await self.get_search_suggestions(search_request.query, 5)
        )
    
    async def _full_text_search(
        self, search_request: ContentSearchRequest
    ) -> List[Dict[str, Any]]:
        """ì „ë¬¸ ê²€ìƒ‰"""
        
        query = {
            "bool": {
                "must": [
                    {
                        "multi_match": {
                            "query": search_request.query,
                            "fields": [
                                "title^3",      # ì œëª©ì— 3ë°° ê°€ì¤‘ì¹˜
                                "summary^2",    # ìš”ì•½ì— 2ë°° ê°€ì¤‘ì¹˜
                                "content",
                                "tags^1.5",     # íƒœê·¸ì— 1.5ë°° ê°€ì¤‘ì¹˜
                                "ai_summary"
                            ],
                            "type": "best_fields",
                            "fuzziness": "AUTO" if search_request.fuzzy else "0"
                        }
                    }
                ],
                "filter": []
            }
        }
        
        # í•„í„° ì ìš©
        if search_request.filters:
            filters = await self._build_filters(search_request.filters)
            query["bool"]["filter"].extend(filters)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        response = await self.es.search(
            index=self.index_name,
            body={
                "query": query,
                "size": search_request.max_results,
                "sort": [
                    {"_score": {"order": "desc"}},
                    {"published_at": {"order": "desc"}}
                ],
                "highlight": {
                    "fields": {
                        "title": {},
                        "content": {"fragment_size": 150, "number_of_fragments": 3},
                        "summary": {}
                    }
                } if search_request.highlight else {}
            }
        )
        
        return self._process_search_results(response)
    
    async def _semantic_search(
        self, search_request: ContentSearchRequest
    ) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„)"""
        
        # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
        query_vector = await self._generate_query_vector(search_request.query)
        
        # ë²¡í„° ê²€ìƒ‰
        vector_query = {
            "script_score": {
                "query": {"match_all": {}},
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'content_vector') + 1.0",
                    "params": {"query_vector": query_vector}
                }
            }
        }
        
        # í•„í„° ì ìš©
        if search_request.filters:
            filters = await self._build_filters(search_request.filters)
            vector_query = {
                "bool": {
                    "must": [vector_query],
                    "filter": filters
                }
            }
        
        response = await self.es.search(
            index=self.vector_index,
            body={
                "query": vector_query,
                "size": search_request.max_results,
                "_source": ["content_id", "title", "summary"]
            }
        )
        
        # ì½˜í…ì¸  IDë¡œ ì „ì²´ ì •ë³´ ì¡°íšŒ
        content_ids = [hit["_source"]["content_id"] for hit in response["hits"]["hits"]]
        return await self._get_contents_by_ids(content_ids)
    
    async def _hybrid_search(
        self, search_request: ContentSearchRequest
    ) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì „ë¬¸ + ì˜ë¯¸)"""
        
        # ë³‘ë ¬ë¡œ ë‘ ê²€ìƒ‰ ì‹¤í–‰
        full_text_results = await self._full_text_search(search_request)
        semantic_results = await self._semantic_search(search_request)
        
        # ê²°ê³¼ í†µí•© ë° ë¦¬ë­í‚¹
        combined_results = await self._combine_and_rerank(
            full_text_results, semantic_results, search_request.query
        )
        
        return combined_results[:search_request.max_results]
    
    async def _combine_and_rerank(
        self, full_text_results: List[Dict], semantic_results: List[Dict], query: str
    ) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë° ë¦¬ë­í‚¹"""
        
        # ê²°ê³¼ë¥¼ ID ê¸°ì¤€ìœ¼ë¡œ í†µí•©
        combined_scores = {}
        
        # ì „ë¬¸ ê²€ìƒ‰ ì ìˆ˜ (0.6 ê°€ì¤‘ì¹˜)
        for i, result in enumerate(full_text_results):
            content_id = result["id"]
            position_score = 1.0 - (i / len(full_text_results))
            combined_scores[content_id] = {
                "content": result,
                "full_text_score": result.get("score", 0) * 0.6,
                "semantic_score": 0,
                "position_bonus": position_score * 0.1
            }
        
        # ì˜ë¯¸ ê²€ìƒ‰ ì ìˆ˜ (0.4 ê°€ì¤‘ì¹˜)
        for i, result in enumerate(semantic_results):
            content_id = result["id"]
            position_score = 1.0 - (i / len(semantic_results))
            
            if content_id in combined_scores:
                combined_scores[content_id]["semantic_score"] = result.get("score", 0) * 0.4
            else:
                combined_scores[content_id] = {
                    "content": result,
                    "full_text_score": 0,
                    "semantic_score": result.get("score", 0) * 0.4,
                    "position_bonus": position_score * 0.1
                }
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        for content_id, scores in combined_scores.items():
            total_score = (
                scores["full_text_score"] +
                scores["semantic_score"] +
                scores["position_bonus"]
            )
            scores["final_score"] = total_score
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_results = sorted(
            combined_scores.values(),
            key=lambda x: x["final_score"],
            reverse=True
        )
        
        return [item["content"] for item in sorted_results]
    
    async def index_content(self, content: Dict[str, Any]):
        """ì½˜í…ì¸  ì¸ë±ì‹±"""
        
        # ê¸°ë³¸ ë¬¸ì„œ ì¸ë±ì‹±
        doc = {
            "title": content["title"],
            "content": content["content"],
            "summary": content.get("summary", ""),
            "content_type": content["content_type"],
            "primary_category": content.get("primary_category"),
            "secondary_categories": content.get("secondary_categories", []),
            "tags": content.get("tags", []),
            "author": content.get("author"),
            "status": content["status"],
            "published_at": content.get("published_at"),
            "created_at": content["created_at"],
            "quality_score": content.get("quality_score"),
            "view_count": content.get("view_count", 0),
            "like_count": content.get("like_count", 0)
        }
        
        await self.es.index(
            index=self.index_name,
            id=content["id"],
            body=doc
        )
        
        # ë²¡í„° ì¸ë±ì‹±
        content_vector = await self._generate_content_vector(content)
        vector_doc = {
            "content_id": content["id"],
            "title": content["title"],
            "summary": content.get("summary", ""),
            "content_vector": content_vector
        }
        
        await self.es.index(
            index=self.vector_index,
            id=content["id"],
            body=vector_doc
        )
    
    async def get_search_suggestions(
        self, query: str, limit: int = 10
    ) -> List[str]:
        """ê²€ìƒ‰ì–´ ìë™ì™„ì„±"""
        
        # íƒœê·¸ ê¸°ë°˜ ì œì•ˆ
        tag_suggestions = await self._get_tag_suggestions(query, limit // 2)
        
        # ì¸ê¸° ê²€ìƒ‰ì–´ ê¸°ë°˜ ì œì•ˆ
        popular_suggestions = await self._get_popular_suggestions(query, limit // 2)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        all_suggestions = list(set(tag_suggestions + popular_suggestions))
        all_suggestions.sort(key=lambda x: len(x))
        
        return all_suggestions[:limit]
    
    async def get_trending_searches(
        self, period: str = "7d", limit: int = 20
    ) -> List[Dict[str, Any]]:
        """ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ"""
        
        # ê¸°ê°„ë³„ ê²€ìƒ‰ ë¡œê·¸ ì§‘ê³„
        end_date = datetime.utcnow()
        if period == "1d":
            start_date = end_date - timedelta(days=1)
        elif period == "7d":
            start_date = end_date - timedelta(days=7)
        elif period == "30d":
            start_date = end_date - timedelta(days=30)
        else:
            start_date = end_date - timedelta(days=7)
        
        # Elasticsearch ì§‘ê³„ ì¿¼ë¦¬
        query = {
            "bool": {
                "filter": [
                    {
                        "range": {
                            "search_time": {
                                "gte": start_date.isoformat(),
                                "lte": end_date.isoformat()
                            }
                        }
                    }
                ]
            }
        }
        
        aggs = {
            "trending_queries": {
                "terms": {
                    "field": "query.keyword",
                    "size": limit,
                    "order": {"_count": "desc"}
                },
                "aggs": {
                    "unique_users": {
                        "cardinality": {
                            "field": "user_id"
                        }
                    }
                }
            }
        }
        
        response = await self.es.search(
            index="search_logs",
            body={
                "query": query,
                "aggs": aggs,
                "size": 0
            }
        )
        
        trending = []
        for bucket in response["aggregations"]["trending_queries"]["buckets"]:
            trending.append({
                "query": bucket["key"],
                "search_count": bucket["doc_count"],
                "unique_users": bucket["unique_users"]["value"],
                "period": period
            })
        
        return trending
```

## ğŸ¯ ì¶”ì²œ ì‹œìŠ¤í…œ

### í˜‘ì—… í•„í„°ë§ ì¶”ì²œ
```python
# app/domains/contents/services/recommendation_service.py

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from typing import List, Dict, Any, Optional, Tuple
import asyncio
import logging

class ContentRecommendationService:
    """ì½˜í…ì¸  ì¶”ì²œ ì„œë¹„ìŠ¤"""
    
    def __init__(self, content_service, user_service, cache_service):
        self.content_service = content_service
        self.user_service = user_service
        self.cache = cache_service
        
        # ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ê°€ì¤‘ì¹˜
        self.algorithm_weights = {
            "collaborative": 0.4,
            "content_based": 0.3,
            "popularity": 0.2,
            "trending": 0.1
        }
        
        # ë²¡í„°í™” ë„êµ¬
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        self.svd = TruncatedSVD(n_components=100, random_state=42)
        
        # ìºì‹œ ì„¤ì •
        self.recommendation_cache_ttl = 3600  # 1ì‹œê°„
        self.model_cache_ttl = 86400  # 24ì‹œê°„
    
    async def get_recommendations(
        self, request: ContentRecommendationRequest
    ) -> List[RecommendationResponse]:
        """í†µí•© ì¶”ì²œ ì‹œìŠ¤í…œ"""
        
        # ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(request)
        cached_result = await self.cache.get(cache_key)
        if cached_result:
            return cached_result
        
        recommendations = []
        
        if request.recommendation_type == "personalized":
            recommendations = await self._get_personalized_recommendations(request)
        elif request.recommendation_type == "similar":
            recommendations = await self._get_similar_content_recommendations(request)
        elif request.recommendation_type == "popular":
            recommendations = await self._get_popular_recommendations(request)
        elif request.recommendation_type == "trending":
            recommendations = await self._get_trending_recommendations(request)
        else:
            # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ
            recommendations = await self._get_hybrid_recommendations(request)
        
        # ê²°ê³¼ í•„í„°ë§ ë° ì •ë ¬
        filtered_recommendations = await self._filter_and_rank(recommendations, request)
        
        # ìºì‹œ ì €ì¥
        await self.cache.set(cache_key, filtered_recommendations, self.recommendation_cache_ttl)
        
        return filtered_recommendations
    
    async def _get_personalized_recommendations(
        self, request: ContentRecommendationRequest
    ) -> List[RecommendationResponse]:
        """ê°œì¸í™” ì¶”ì²œ (í˜‘ì—… í•„í„°ë§ + ì½˜í…ì¸  ê¸°ë°˜)"""
        
        if not request.user_id:
            return await self._get_popular_recommendations(request)
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ë¶„ì„
        user_profile = await self._analyze_user_profile(request.user_id)
        
        # ë³‘ë ¬ ì¶”ì²œ ì‹¤í–‰
        tasks = [
            self._collaborative_filtering(request.user_id, user_profile),
            self._content_based_filtering(request.user_id, user_profile),
            self._popularity_based_filtering(user_profile),
            self._trending_based_filtering(user_profile)
        ]
        
        collaborative_recs, content_recs, popularity_recs, trending_recs = await asyncio.gather(*tasks)
        
        # ê°€ì¤‘ ê²°í•©
        combined_recommendations = await self._combine_recommendations([
            (collaborative_recs, self.algorithm_weights["collaborative"]),
            (content_recs, self.algorithm_weights["content_based"]),
            (popularity_recs, self.algorithm_weights["popularity"]),
            (trending_recs, self.algorithm_weights["trending"])
        ])
        
        return combined_recommendations[:request.limit]
    
    async def _collaborative_filtering(
        self, user_id: str, user_profile: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """í˜‘ì—… í•„í„°ë§ ì¶”ì²œ"""
        
        # ì‚¬ìš©ì-ì½˜í…ì¸  ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        interaction_matrix = await self._build_interaction_matrix()
        
        if interaction_matrix is None or len(interaction_matrix) == 0:
            return []
        
        # ì‚¬ìš©ì ìœ ì‚¬ë„ ê³„ì‚°
        user_similarities = await self._calculate_user_similarities(user_id, interaction_matrix)
        
        # ìœ ì‚¬ ì‚¬ìš©ìë“¤ì˜ ì„ í˜¸ ì½˜í…ì¸  ì¶”ì²œ
        recommendations = []
        similar_users = sorted(user_similarities.items(), key=lambda x: x[1], reverse=True)[:20]
        
        for similar_user_id, similarity in similar_users:
            if similarity < 0.1:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                continue
            
            # ìœ ì‚¬ ì‚¬ìš©ìì˜ ê³ í‰ê°€ ì½˜í…ì¸  ì¡°íšŒ
            user_contents = await self._get_user_high_rated_contents(similar_user_id)
            
            for content in user_contents:
                # ì´ë¯¸ ë³¸ ì½˜í…ì¸  ì œì™¸
                if not await self._user_has_seen_content(user_id, content["id"]):
                    score = similarity * content.get("rating", 0.5)
                    recommendations.append({
                        "content": content,
                        "score": score,
                        "reason": f"ìœ ì‚¬í•œ ê´€ì‹¬ì‚¬ë¥¼ ê°€ì§„ ì‚¬ìš©ìë“¤ì´ ì„ í˜¸í•¨",
                        "algorithm": "collaborative"
                    })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        recommendations.sort(key=lambda x: x["score"], reverse=True)
        return recommendations
    
    async def _content_based_filtering(
        self, user_id: str, user_profile: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§"""
        
        # ì‚¬ìš©ìê°€ ì„ í˜¸í•˜ëŠ” ì½˜í…ì¸  íŠ¹ì„± ë¶„ì„
        user_preferences = await self._extract_user_preferences(user_id)
        
        # ëª¨ë“  ì½˜í…ì¸ ì˜ íŠ¹ì„± ë²¡í„° ì¡°íšŒ
        content_features = await self._get_content_feature_vectors()
        
        if not user_preferences or not content_features:
            return []
        
        # ì‚¬ìš©ì ì„ í˜¸ë„ ë²¡í„°ì™€ ì½˜í…ì¸  íŠ¹ì„± ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        user_vector = user_preferences["feature_vector"]
        recommendations = []
        
        for content_id, feature_vector in content_features.items():
            # ì´ë¯¸ ë³¸ ì½˜í…ì¸  ì œì™¸
            if await self._user_has_seen_content(user_id, content_id):
                continue
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity([user_vector], [feature_vector])[0][0]
            
            if similarity > 0.3:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                content = await self.content_service.get_content_by_id(content_id)
                if content:
                    recommendations.append({
                        "content": content,
                        "score": similarity,
                        "reason": f"ì„ í˜¸í•˜ëŠ” {', '.join(user_preferences['top_categories'])} ë¶„ì•¼ì™€ ìœ ì‚¬",
                        "algorithm": "content_based"
                    })
        
        recommendations.sort(key=lambda x: x["score"], reverse=True)
        return recommendations
    
    async def _get_similar_content_recommendations(
        self, request: ContentRecommendationRequest
    ) -> List[RecommendationResponse]:
        """ìœ ì‚¬ ì½˜í…ì¸  ì¶”ì²œ"""
        
        if not request.content_id:
            raise ValueError("content_id is required for similar content recommendations")
        
        # ê¸°ì¤€ ì½˜í…ì¸  ì¡°íšŒ
        base_content = await self.content_service.get_content_by_id(request.content_id)
        if not base_content:
            return []
        
        # ìœ ì‚¬ë„ ê³„ì‚° ë°©ì‹ë³„ ì¶”ì²œ
        tasks = [
            self._find_similar_by_tags(base_content),
            self._find_similar_by_category(base_content),
            self._find_similar_by_content(base_content),
            self._find_similar_by_user_behavior(base_content)
        ]
        
        tag_similar, category_similar, content_similar, behavior_similar = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        all_similar = {}
        
        # íƒœê·¸ ìœ ì‚¬ë„ (0.3 ê°€ì¤‘ì¹˜)
        for item in tag_similar:
            content_id = item["content"]["id"]
            all_similar[content_id] = {
                "content": item["content"],
                "score": item["score"] * 0.3,
                "reasons": [item["reason"]]
            }
        
        # ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ (0.2 ê°€ì¤‘ì¹˜)
        for item in category_similar:
            content_id = item["content"]["id"]
            if content_id in all_similar:
                all_similar[content_id]["score"] += item["score"] * 0.2
                all_similar[content_id]["reasons"].append(item["reason"])
            else:
                all_similar[content_id] = {
                    "content": item["content"],
                    "score": item["score"] * 0.2,
                    "reasons": [item["reason"]]
                }
        
        # ì½˜í…ì¸  ìœ ì‚¬ë„ (0.4 ê°€ì¤‘ì¹˜)
        for item in content_similar:
            content_id = item["content"]["id"]
            if content_id in all_similar:
                all_similar[content_id]["score"] += item["score"] * 0.4
                all_similar[content_id]["reasons"].append(item["reason"])
            else:
                all_similar[content_id] = {
                    "content": item["content"],
                    "score": item["score"] * 0.4,
                    "reasons": [item["reason"]]
                }
        
        # ì‚¬ìš©ì í–‰ë™ ìœ ì‚¬ë„ (0.1 ê°€ì¤‘ì¹˜)
        for item in behavior_similar:
            content_id = item["content"]["id"]
            if content_id in all_similar:
                all_similar[content_id]["score"] += item["score"] * 0.1
                all_similar[content_id]["reasons"].append(item["reason"])
            else:
                all_similar[content_id] = {
                    "content": item["content"],
                    "score": item["score"] * 0.1,
                    "reasons": [item["reason"]]
                }
        
        # ìµœì¢… ì¶”ì²œ ìƒì„±
        recommendations = []
        for content_id, data in all_similar.items():
            recommendations.append(RecommendationResponse(
                content=data["content"],
                score=data["score"],
                reasons=data["reasons"][:2],  # ìƒìœ„ 2ê°œ ì´ìœ ë§Œ
                algorithm="similar_content"
            ))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        recommendations.sort(key=lambda x: x.score, reverse=True)
        return recommendations[:request.limit]
    
    async def _find_similar_by_tags(self, base_content: Dict[str, Any]) -> List[Dict[str, Any]]:
        """íƒœê·¸ ê¸°ë°˜ ìœ ì‚¬ ì½˜í…ì¸  ê²€ìƒ‰"""
        base_tags = set(base_content.get("tags", []))
        if not base_tags:
            return []
        
        # íƒœê·¸ê°€ ê²¹ì¹˜ëŠ” ì½˜í…ì¸  ê²€ìƒ‰
        similar_contents = await self.content_service.find_contents_by_tags(list(base_tags))
        
        recommendations = []
        for content in similar_contents:
            if content["id"] == base_content["id"]:
                continue
            
            content_tags = set(content.get("tags", []))
            overlap = len(base_tags & content_tags)
            similarity = overlap / len(base_tags | content_tags)
            
            if similarity > 0.2:
                recommendations.append({
                    "content": content,
                    "score": similarity,
                    "reason": f"{overlap}ê°œì˜ ê³µí†µ íƒœê·¸"
                })
        
        return recommendations
    
    async def _find_similar_by_content(self, base_content: Dict[str, Any]) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ë‚´ìš© ê¸°ë°˜ ìœ ì‚¬ ì½˜í…ì¸  ê²€ìƒ‰"""
        
        # ê¸°ì¤€ ì½˜í…ì¸ ì˜ í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ
        base_text = f"{base_content['title']} {base_content.get('summary', '')} {base_content['content'][:500]}"
        
        # ë‹¤ë¥¸ ì½˜í…ì¸ ë“¤ê³¼ TF-IDF ìœ ì‚¬ë„ ê³„ì‚°
        all_contents = await self.content_service.get_published_contents(limit=1000)
        
        texts = [base_text]
        content_map = {}
        
        for content in all_contents:
            if content["id"] == base_content["id"]:
                continue
            
            text = f"{content['title']} {content.get('summary', '')} {content['content'][:500]}"
            texts.append(text)
            content_map[len(texts) - 1] = content
        
        if len(texts) < 2:
            return []
        
        # TF-IDF ë²¡í„°í™”
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()
        
        recommendations = []
        for i, similarity in enumerate(similarities):
            if similarity > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                content = content_map[i + 1]
                recommendations.append({
                    "content": content,
                    "score": similarity,
                    "reason": f"ë‚´ìš© ìœ ì‚¬ë„ {similarity:.2f}"
                })
        
        return recommendations
    
    async def _build_interaction_matrix(self) -> Optional[np.ndarray]:
        """ì‚¬ìš©ì-ì½˜í…ì¸  ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±"""
        
        # ìºì‹œ í™•ì¸
        cached_matrix = await self.cache.get("interaction_matrix")
        if cached_matrix:
            return np.array(cached_matrix)
        
        # ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë°ì´í„° ìˆ˜ì§‘
        interactions = await self.user_service.get_all_user_interactions()
        
        if not interactions:
            return None
        
        # ì‚¬ìš©ì ë° ì½˜í…ì¸  ID ë§¤í•‘
        user_ids = list(set(interaction["user_id"] for interaction in interactions))
        content_ids = list(set(interaction["content_id"] for interaction in interactions))
        
        user_id_map = {user_id: i for i, user_id in enumerate(user_ids)}
        content_id_map = {content_id: i for i, content_id in enumerate(content_ids)}
        
        # ë§¤íŠ¸ë¦­ìŠ¤ ì´ˆê¸°í™”
        matrix = np.zeros((len(user_ids), len(content_ids)))
        
        # ìƒí˜¸ì‘ìš© ì ìˆ˜ ê³„ì‚° ë° ë§¤íŠ¸ë¦­ìŠ¤ ì±„ìš°ê¸°
        for interaction in interactions:
            user_idx = user_id_map[interaction["user_id"]]
            content_idx = content_id_map[interaction["content_id"]]
            
            # ìƒí˜¸ì‘ìš© íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
            score = 0
            if interaction["type"] == "view":
                score += 1
            elif interaction["type"] == "like":
                score += 3
            elif interaction["type"] == "share":
                score += 5
            elif interaction["type"] == "bookmark":
                score += 4
            
            # ì½ê¸° ì‹œê°„ ê³ ë ¤
            if "read_time" in interaction:
                read_ratio = min(interaction["read_time"] / interaction.get("estimated_read_time", 300), 1.0)
                score *= (0.5 + 0.5 * read_ratio)
            
            matrix[user_idx][content_idx] = max(matrix[user_idx][content_idx], score)
        
        # ë§¤íŠ¸ë¦­ìŠ¤ ìºì‹±
        await self.cache.set("interaction_matrix", matrix.tolist(), self.model_cache_ttl)
        
        return matrix
    
    def _generate_cache_key(self, request: ContentRecommendationRequest) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_parts = [
            f"user:{request.user_id or 'anonymous'}",
            f"type:{request.recommendation_type}",
            f"content:{request.content_id or 'none'}",
            f"limit:{request.limit}"
        ]
        
        if request.preferred_categories:
            key_parts.append(f"cats:{'|'.join(sorted(request.preferred_categories))}")
        
        if request.preferred_tags:
            key_parts.append(f"tags:{'|'.join(sorted(request.preferred_tags))}")
        
        return f"recommendations:{'|'.join(key_parts)}"
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­

### ì„±ëŠ¥ ì§€í‘œ ì¶”ì 
```python
# app/domains/contents/monitoring/metrics.py

from prometheus_client import Counter, Histogram, Gauge
import time
from functools import wraps

# ë©”íŠ¸ë¦­ ì •ì˜
content_requests_total = Counter(
    'content_api_requests_total',
    'Total content API requests',
    ['method', 'endpoint', 'status']
)

content_request_duration = Histogram(
    'content_api_request_duration_seconds',
    'Content API request duration',
    ['method', 'endpoint']
)

search_requests_total = Counter(
    'content_search_requests_total',
    'Total content search requests',
    ['search_type', 'status']
)

search_duration = Histogram(
    'content_search_duration_seconds',
    'Content search duration',
    ['search_type']
)

recommendation_requests_total = Counter(
    'content_recommendation_requests_total',
    'Total recommendation requests',
    ['recommendation_type', 'status']
)

content_classification_accuracy = Gauge(
    'content_classification_accuracy',
    'Content classification accuracy'
)

content_quality_score = Histogram(
    'content_quality_score',
    'Content quality scores',
    buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

def monitor_content_api(endpoint_name: str):
    """ì½˜í…ì¸  API ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            status = "success"
            
            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                status = "error"
                raise
            finally:
                duration = time.time() - start_time
                
                content_requests_total.labels(
                    method="GET",
                    endpoint=endpoint_name,
                    status=status
                ).inc()
                
                content_request_duration.labels(
                    method="GET",
                    endpoint=endpoint_name
                ).observe(duration)
        
        return wrapper
    return decorator
```

## ğŸ”„ ì—…ë°ì´íŠ¸ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ì‚¬í•­ | ì‘ì„±ì |
|------|------|----------|--------|
| 1.0.0 | 2025-08-14 | ì´ˆê¸° Content ë„ë©”ì¸ êµ¬í˜„ ë¬¸ì„œ ì‘ì„± | Backend Team |

---

*Content ë„ë©”ì¸ì€ Korea Public Data í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ì½˜í…ì¸  ê´€ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ, AI ê¸°ë°˜ ìë™í™”ì™€ ì§€ëŠ¥í˜• ê²€ìƒ‰ì„ í†µí•´ ì‚¬ìš©ìì—ê²Œ ìµœì ì˜ ì½˜í…ì¸  ê²½í—˜ì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.*