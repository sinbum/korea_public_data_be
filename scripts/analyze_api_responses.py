#!/usr/bin/env python3
"""
K-Startup API ì‘ë‹µ ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ì‹¤ì œ API ì‘ë‹µ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë°ì´í„° êµ¬ì¡°, í•„ë“œ ë§¤í•‘, í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, List, Set
from collections import defaultdict, Counter
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
RESULTS_DIR = Path("tests/live/results")
ANALYSIS_OUTPUT_DIR = Path("docs/reports/api_analysis")
ANALYSIS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


class APIResponseAnalyzer:
    """API ì‘ë‹µ ë°ì´í„° ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.sample_files = {
            "announcements": "announcements_basic_sample.json",
            "business": "business_basic_sample.json", 
            "content": "content_basic_sample.json",
            "statistics": "statistics_basic_sample.json"
        }
        self.analysis_results = {}
        
    def analyze_all_endpoints(self):
        """ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„"""
        logger.info("Starting comprehensive API response analysis...")
        
        for endpoint_name, sample_file in self.sample_files.items():
            sample_path = RESULTS_DIR / sample_file
            
            if sample_path.exists():
                logger.info(f"Analyzing {endpoint_name} endpoint data...")
                analysis = self.analyze_endpoint_data(endpoint_name, sample_path)
                self.analysis_results[endpoint_name] = analysis
            else:
                logger.warning(f"Sample file not found: {sample_path}")
        
        # ì¢…í•© ë¶„ì„ ìˆ˜í–‰
        self.perform_cross_endpoint_analysis()
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.save_analysis_results()
        
        logger.info("Analysis completed successfully")
    
    def analyze_endpoint_data(self, endpoint_name: str, sample_path: Path) -> Dict[str, Any]:
        """ê°œë³„ ì—”ë“œí¬ì¸íŠ¸ ë°ì´í„° ë¶„ì„"""
        with open(sample_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        analysis = {
            "endpoint": endpoint_name,
            "metadata": self.analyze_metadata(data),
            "data_structure": self.analyze_data_structure(data),
            "field_analysis": self.analyze_fields(data.get("data", [])),
            "data_quality": self.analyze_data_quality(data.get("data", [])),
            "sample_count": len(data.get("data", []))
        }
        
        return analysis
    
    def analyze_metadata(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ë¶„ì„"""
        metadata_fields = ["current_count", "match_count", "page", "per_page", "total_count"]
        metadata = {}
        
        for field in metadata_fields:
            if field in data:
                metadata[field] = {
                    "value": data[field],
                    "type": type(data[field]).__name__
                }
        
        return metadata
    
    def analyze_data_structure(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° êµ¬ì¡° ë¶„ì„"""
        structure = {
            "root_keys": list(data.keys()),
            "has_data_array": "data" in data and isinstance(data["data"], list),
            "data_array_length": len(data.get("data", [])),
            "pagination_present": any(key in data for key in ["page", "per_page", "total_count"])
        }
        
        if structure["has_data_array"] and data["data"]:
            first_item = data["data"][0]
            structure["item_fields"] = list(first_item.keys()) if isinstance(first_item, dict) else []
            structure["item_field_count"] = len(structure["item_fields"])
        
        return structure
    
    def analyze_fields(self, data_items: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í•„ë“œ ë¶„ì„"""
        if not data_items:
            return {"error": "No data items to analyze"}
        
        all_fields = set()
        field_types = defaultdict(Counter)
        field_presence = defaultdict(int)
        field_lengths = defaultdict(list)
        null_counts = defaultdict(int)
        
        for item in data_items:
            if not isinstance(item, dict):
                continue
                
            all_fields.update(item.keys())
            
            for field, value in item.items():
                field_presence[field] += 1
                
                if value is None or value == "":
                    null_counts[field] += 1
                else:
                    field_types[field][type(value).__name__] += 1
                    
                    if isinstance(value, str):
                        field_lengths[field].append(len(value))
        
        # í•„ë“œë³„ í†µê³„ ê³„ì‚°
        field_stats = {}
        for field in all_fields:
            stats = {
                "presence_rate": field_presence[field] / len(data_items) * 100,
                "null_rate": null_counts[field] / len(data_items) * 100,
                "types": dict(field_types[field]),
                "most_common_type": field_types[field].most_common(1)[0][0] if field_types[field] else "null"
            }
            
            if field_lengths[field]:
                stats["string_length_stats"] = {
                    "min": min(field_lengths[field]),
                    "max": max(field_lengths[field]),
                    "avg": sum(field_lengths[field]) / len(field_lengths[field])
                }
            
            field_stats[field] = stats
        
        return {
            "total_unique_fields": len(all_fields),
            "field_statistics": field_stats,
            "all_fields": sorted(list(all_fields))
        }
    
    def analyze_data_quality(self, data_items: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        if not data_items:
            return {"error": "No data items to analyze"}
        
        quality_issues = []
        
        # ì¤‘ë³µ ê²€ì‚¬ (ID ê¸°ì¤€)
        ids = []
        for item in data_items:
            if isinstance(item, dict):
                item_id = item.get("announcement_id") or item.get("business_id") or item.get("id")
                if item_id:
                    ids.append(str(item_id))
        
        duplicate_ids = [item_id for item_id, count in Counter(ids).items() if count > 1]
        if duplicate_ids:
            quality_issues.append(f"Duplicate IDs found: {duplicate_ids}")
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
        required_fields_by_type = {
            "announcements": ["announcement_id", "title", "start_date", "end_date"],
            "business": ["business_id", "business_name"],
            "content": ["title"],
            "statistics": ["title"]
        }
        
        # í•„ë“œ ì™„ì„±ë„ ê²€ì‚¬
        field_completeness = {}
        for item in data_items:
            if isinstance(item, dict):
                for field, value in item.items():
                    if field not in field_completeness:
                        field_completeness[field] = {"filled": 0, "total": 0}
                    
                    field_completeness[field]["total"] += 1
                    if value is not None and value != "":
                        field_completeness[field]["filled"] += 1
        
        # ì™„ì„±ë„ ë‚®ì€ í•„ë“œ ì‹ë³„
        low_completeness_fields = []
        for field, stats in field_completeness.items():
            completeness = stats["filled"] / stats["total"] * 100
            if completeness < 50:  # 50% ë¯¸ë§Œ ì™„ì„±ë„
                low_completeness_fields.append({
                    "field": field,
                    "completeness": completeness
                })
        
        return {
            "total_items_analyzed": len(data_items),
            "duplicate_ids": duplicate_ids,
            "quality_issues": quality_issues,
            "field_completeness": field_completeness,
            "low_completeness_fields": low_completeness_fields
        }
    
    def perform_cross_endpoint_analysis(self):
        """ì—”ë“œí¬ì¸íŠ¸ ê°„ êµì°¨ ë¶„ì„"""
        logger.info("Performing cross-endpoint analysis...")
        
        # ê³µí†µ í•„ë“œ ì°¾ê¸°
        all_endpoint_fields = {}
        for endpoint, analysis in self.analysis_results.items():
            if "field_analysis" in analysis:
                all_endpoint_fields[endpoint] = set(analysis["field_analysis"].get("all_fields", []))
        
        common_fields = set.intersection(*all_endpoint_fields.values()) if all_endpoint_fields else set()
        
        # ë°ì´í„° êµ¬ì¡° ì¼ê´€ì„± ê²€ì‚¬
        structure_consistency = {}
        for endpoint, analysis in self.analysis_results.items():
            if "data_structure" in analysis:
                structure = analysis["data_structure"]
                structure_consistency[endpoint] = {
                    "has_pagination": structure.get("pagination_present", False),
                    "has_data_array": structure.get("has_data_array", False),
                    "field_count": structure.get("item_field_count", 0)
                }
        
        self.analysis_results["cross_endpoint_analysis"] = {
            "common_fields": sorted(list(common_fields)),
            "structure_consistency": structure_consistency,
            "total_endpoints_analyzed": len(self.analysis_results)
        }
    
    def save_analysis_results(self):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        # JSON í˜•íƒœë¡œ ìƒì„¸ ê²°ê³¼ ì €ì¥
        results_file = ANALYSIS_OUTPUT_DIR / "api_response_analysis_detailed.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2, default=str)
        
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_markdown_report()
        
        logger.info(f"Analysis results saved to {ANALYSIS_OUTPUT_DIR}")
    
    def generate_markdown_report(self):
        """ë§ˆí¬ë‹¤ìš´ í˜•íƒœì˜ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_lines = [
            "# K-Startup API ì‘ë‹µ ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸",
            "",
            f"ë¶„ì„ ì¼ì‹œ: {self._get_current_timestamp()}",
            "",
            "## ğŸ“Š ê°œìš”",
            "",
            f"- ë¶„ì„ëœ ì—”ë“œí¬ì¸íŠ¸ ìˆ˜: {len([k for k in self.analysis_results.keys() if k != 'cross_endpoint_analysis'])}",
            f"- ì´ ìƒ˜í”Œ ë°ì´í„°: {sum(analysis.get('sample_count', 0) for analysis in self.analysis_results.values() if isinstance(analysis.get('sample_count'), int))}ê°œ",
            ""
        ]
        
        # ì—”ë“œí¬ì¸íŠ¸ë³„ ë¶„ì„ ê²°ê³¼
        for endpoint, analysis in self.analysis_results.items():
            if endpoint == "cross_endpoint_analysis":
                continue
                
            report_lines.extend([
                f"## ğŸ” {endpoint.title()} ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„",
                "",
                "### ë©”íƒ€ë°ì´í„°",
                ""
            ])
            
            if "metadata" in analysis:
                for field, info in analysis["metadata"].items():
                    report_lines.append(f"- **{field}**: {info['value']} ({info['type']})")
                report_lines.append("")
            
            # ë°ì´í„° êµ¬ì¡°
            if "data_structure" in analysis:
                structure = analysis["data_structure"]
                report_lines.extend([
                    "### ë°ì´í„° êµ¬ì¡°",
                    "",
                    f"- ë£¨íŠ¸ í‚¤: {', '.join(structure.get('root_keys', []))}",
                    f"- ë°ì´í„° ë°°ì—´ ì¡´ì¬: {'âœ…' if structure.get('has_data_array') else 'âŒ'}",
                    f"- í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›: {'âœ…' if structure.get('pagination_present') else 'âŒ'}",
                    f"- ì•„ì´í…œë‹¹ í•„ë“œ ìˆ˜: {structure.get('item_field_count', 0)}ê°œ",
                    ""
                ])
            
            # í•„ë“œ ë¶„ì„
            if "field_analysis" in analysis and "field_statistics" in analysis["field_analysis"]:
                report_lines.extend([
                    "### ì£¼ìš” í•„ë“œ ë¶„ì„",
                    "",
                    "| í•„ë“œëª… | ì™„ì„±ë„ | ì£¼ìš” íƒ€ì… | ë¹„ê³  |",
                    "|--------|--------|----------|------|"
                ])
                
                field_stats = analysis["field_analysis"]["field_statistics"]
                for field, stats in sorted(field_stats.items())[:10]:  # ìƒìœ„ 10ê°œ í•„ë“œë§Œ í‘œì‹œ
                    presence_rate = f"{stats['presence_rate']:.1f}%"
                    main_type = stats['most_common_type']
                    
                    note = ""
                    if stats['null_rate'] > 50:
                        note = "âš ï¸ ë†’ì€ NULL ë¹„ìœ¨"
                    elif stats['null_rate'] == 0:
                        note = "âœ… ì™„ì „"
                    
                    report_lines.append(f"| {field} | {presence_rate} | {main_type} | {note} |")
                
                report_lines.append("")
            
            # ë°ì´í„° í’ˆì§ˆ
            if "data_quality" in analysis:
                quality = analysis["data_quality"]
                report_lines.extend([
                    "### ë°ì´í„° í’ˆì§ˆ",
                    "",
                    f"- ë¶„ì„ëœ ì•„ì´í…œ ìˆ˜: {quality.get('total_items_analyzed', 0)}ê°œ",
                    f"- ì¤‘ë³µ ID: {len(quality.get('duplicate_ids', []))}ê°œ",
                    f"- í’ˆì§ˆ ì´ìŠˆ: {len(quality.get('quality_issues', []))}ê°œ",
                    ""
                ])
                
                if quality.get('low_completeness_fields'):
                    report_lines.extend([
                        "#### ì™„ì„±ë„ ë‚®ì€ í•„ë“œ (50% ë¯¸ë§Œ)",
                        ""
                    ])
                    for field_info in quality['low_completeness_fields'][:5]:  # ìƒìœ„ 5ê°œë§Œ
                        report_lines.append(f"- **{field_info['field']}**: {field_info['completeness']:.1f}%")
                    report_lines.append("")
        
        # êµì°¨ ë¶„ì„ ê²°ê³¼
        if "cross_endpoint_analysis" in self.analysis_results:
            cross_analysis = self.analysis_results["cross_endpoint_analysis"]
            report_lines.extend([
                "## ğŸ”„ ì—”ë“œí¬ì¸íŠ¸ ê°„ êµì°¨ ë¶„ì„",
                "",
                f"### ê³µí†µ í•„ë“œ ({len(cross_analysis.get('common_fields', []))}ê°œ)",
                ""
            ])
            
            if cross_analysis.get('common_fields'):
                for field in cross_analysis['common_fields']:
                    report_lines.append(f"- `{field}`")
                report_lines.append("")
            
            # êµ¬ì¡° ì¼ê´€ì„±
            if cross_analysis.get('structure_consistency'):
                report_lines.extend([
                    "### êµ¬ì¡° ì¼ê´€ì„±",
                    "",
                    "| ì—”ë“œí¬ì¸íŠ¸ | í˜ì´ì§€ë„¤ì´ì…˜ | ë°ì´í„° ë°°ì—´ | í•„ë“œ ìˆ˜ |",
                    "|------------|--------------|-------------|---------|"
                ])
                
                for endpoint, consistency in cross_analysis['structure_consistency'].items():
                    pagination = "âœ…" if consistency.get('has_pagination') else "âŒ"
                    data_array = "âœ…" if consistency.get('has_data_array') else "âŒ"
                    field_count = consistency.get('field_count', 0)
                    
                    report_lines.append(f"| {endpoint} | {pagination} | {data_array} | {field_count} |")
                
                report_lines.append("")
        
        # ê¶Œì¥ì‚¬í•­
        report_lines.extend([
            "## ğŸ’¡ ê¶Œì¥ì‚¬í•­",
            "",
            "### ë°ì´í„° í’ˆì§ˆ ê°œì„ ",
            "- NULL ê°’ì´ ë§ì€ í•„ë“œì— ëŒ€í•œ ë°ì´í„° ë³´ì™„ ê²€í† ",
            "- ì¤‘ë³µ ë°ì´í„° ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•",
            "- í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë¡œì§ ê°•í™”",
            "",
            "### API ì¼ê´€ì„± í–¥ìƒ", 
            "- ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ë™ì¼í•œ í˜ì´ì§€ë„¤ì´ì…˜ êµ¬ì¡° ì‚¬ìš©",
            "- ê³µí†µ ë©”íƒ€ë°ì´í„° í•„ë“œ í‘œì¤€í™”",
            "- ì‘ë‹µ í˜•ì‹ í†µì¼í™”",
            "",
            "### ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦",
            "- ì •ê¸°ì ì¸ ë°ì´í„° í’ˆì§ˆ ê²€ì¦",
            "- API ì‘ë‹µ êµ¬ì¡° ë³€ê²½ ê°ì§€ ì‹œìŠ¤í…œ",
            "- ì‹¤ì‹œê°„ ë°ì´í„° ì™„ì„±ë„ ëª¨ë‹ˆí„°ë§"
        ])
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
        report_file = ANALYSIS_OUTPUT_DIR / "api_response_analysis_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
    
    def _get_current_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” K-Startup API ì‘ë‹µ ë°ì´í„° ë¶„ì„ ì‹œì‘...")
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ë³€ê²½
    os.chdir(Path(__file__).parent.parent)
    
    analyzer = APIResponseAnalyzer()
    analyzer.analyze_all_endpoints()
    
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“„ ë¦¬í¬íŠ¸ ìœ„ì¹˜: {ANALYSIS_OUTPUT_DIR}")
    print(f"   - ìƒì„¸ ê²°ê³¼: api_response_analysis_detailed.json")
    print(f"   - ìš”ì•½ ë¦¬í¬íŠ¸: api_response_analysis_report.md")


if __name__ == "__main__":
    main()